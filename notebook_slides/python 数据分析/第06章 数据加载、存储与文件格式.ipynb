{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f2abfd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "访问数据是使用本书所介绍的这些工具的第一步。我会着重介绍pandas的数据输入与输出，虽然别的库中也有不少以此为目的的工具。\n",
    "\n",
    "输入输出通常可以划分为几个大类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据，利用Web API操作网络资源。\n",
    "\n",
    "# 6.1 读写文本格式的数据\n",
    "pandas提供了一些用于将表格型数据读取为DataFrame对象的函数。表6-1对它们进行了总结，其中read_csv和read_table可能会是你今后用得最多的。\n",
    "\n",
    "![表6-1 pandas中的解析函数](./figures/t6-1.png)\n",
    "\n",
    "我将大致介绍一下这些函数在将文本数据转换为DataFrame时所用到的一些技术。这些函数的选项可以划分为以下几个大类：\n",
    "\n",
    "- 索引：将一个或多个列当做返回的DataFrame处理，以及是否从文件、用户获取列名。\n",
    "- 类型推断和数据转换：包括用户定义值的转换、和自定义的缺失值标记列表等。\n",
    "- 日期解析：包括组合功能，比如将分散在多个列中的日期时间信息组合成结果中的单个列。\n",
    "- 迭代：支持对大文件进行逐块迭代。\n",
    "- 不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西（比如由成千上万个逗号隔开的数值数据）。\n",
    "\n",
    "因为工作中实际碰到的数据可能十分混乱，一些数据加载函数（尤其是read_csv）的选项逐渐变得复杂起来。面对不同的参数，感到头痛很正常（read_csv有超过50个参数）。pandas文档有这些参数的例子，如果你感到阅读某个文件很难，可以通过相似的足够多的例子找到正确的参数。\n",
    "\n",
    "其中一些函数，比如pandas.read_csv，有类型推断功能，因为列数据的类型不属于数据类型。也就是说，你不需要指定列的类型到底是数值、整数、布尔值，还是字符串。其它的数据格式，如HDF5、Feather和msgpack，会在格式中存储数据类型。\n",
    "\n",
    "日期和其他自定义类型的处理需要多花点工夫才行。首先我们来看一个以逗号分隔的（CSV）文本文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0783aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [8]: !cat examples/ex1.csv\n",
    "a,b,c,d,message\n",
    "1,2,3,4,hello\n",
    "5,6,7,8,world\n",
    "9,10,11,12,foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86087631",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    ">笔记：这里，我用的是Unix的cat shell命令将文件的原始内容打印到屏幕上。如果你用的是Windows，你可以使用type达到同样的效果。\n",
    "\n",
    "由于该文件以逗号分隔，所以我们可以使用read_csv将其读入一个DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [9]: df = pd.read_csv('examples/ex1.csv')\n",
    "\n",
    "In [10]: df\n",
    "Out[10]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900783b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "我们还可以使用read_table，并指定分隔符："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aefcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [11]: pd.read_table('examples/ex1.csv', sep=',')\n",
    "Out[11]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c44054",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "并不是所有文件都有标题行。看看下面这个文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [12]: !cat examples/ex2.csv\n",
    "1,2,3,4,hello\n",
    "5,6,7,8,world\n",
    "9,10,11,12,foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159dbd1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "读入该文件的办法有两个。你可以让pandas为其分配默认的列名，也可以自己定义列名："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [13]: pd.read_csv('examples/ex2.csv', header=None)\n",
    "Out[13]: \n",
    "   0   1   2   3      4\n",
    "0  1   2   3   4  hello\n",
    "1  5   6   7   8  world\n",
    "2  9  10  11  12    foo\n",
    "\n",
    "In [14]: pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])\n",
    "Out[14]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b5cd8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "假设你希望将message列做成DataFrame的索引。你可以明确表示要将该列放到索引4的位置上，也可以通过index_col参数指定\"message\"："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [15]: names = ['a', 'b', 'c', 'd', 'message']\n",
    "\n",
    "In [16]: pd.read_csv('examples/ex2.csv', names=names, index_col='message')\n",
    "Out[16]: \n",
    "         a   b   c   d\n",
    "message               \n",
    "hello    1   2   3   4\n",
    "world    5   6   7   8\n",
    "foo      9  10  11  12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9ddbf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果希望将多个列做成一个层次化索引，只需传入由列编号或列名组成的列表即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [17]: !cat examples/csv_mindex.csv\n",
    "key1,key2,value1,value2\n",
    "one,a,1,2\n",
    "one,b,3,4\n",
    "one,c,5,6\n",
    "one,d,7,8\n",
    "two,a,9,10\n",
    "two,b,11,12\n",
    "two,c,13,14\n",
    "two,d,15,16\n",
    "\n",
    "In [18]: parsed = pd.read_csv('examples/csv_mindex.csv',\n",
    "   ....:                      index_col=['key1', 'key2'])\n",
    "\n",
    "In [19]: parsed\n",
    "Out[19]: \n",
    "           value1  value2\n",
    "key1 key2                \n",
    "one  a          1       2\n",
    "     b          3       4\n",
    "     c          5       6\n",
    "     d          7       8\n",
    "two  a          9      10\n",
    "     b         11      12\n",
    "     c         13      14\n",
    "     d         15      16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e03c07",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "有些情况下，有些表格可能不是用固定的分隔符去分隔字段的（比如空白符或其它模式）。看看下面这个文本文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4acd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [20]: list(open('examples/ex3.txt'))\n",
    "Out[20]: \n",
    "['            A         B         C\\n',\n",
    " 'aaa -0.264438 -1.026059 -0.619500\\n',\n",
    " 'bbb  0.927272  0.302904 -0.032399\\n',\n",
    " 'ccc -0.264273 -0.386314 -0.217601\\n',\n",
    " 'ddd -0.871858 -0.348382  1.100491\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4e827",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "虽然可以手动对数据进行规整，这里的字段是被数量不同的空白字符间隔开的。这种情况下，你可以传递一个正则表达式作为read_table的分隔符。可以用正则表达式表达为\\s+，于是有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [21]: result = pd.read_table('examples/ex3.txt', sep='\\s+')\n",
    "\n",
    "In [22]: result\n",
    "Out[22]: \n",
    "            A         B         C\n",
    "aaa -0.264438 -1.026059 -0.619500\n",
    "bbb  0.927272  0.302904 -0.032399\n",
    "ccc -0.264273 -0.386314 -0.217601\n",
    "ddd -0.871858 -0.348382  1.100491"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafd30d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这里，由于列名比数据行的数量少，所以read_table推断第一列应该是DataFrame的索引。\n",
    "\n",
    "这些解析器函数还有许多参数可以帮助你处理各种各样的异形文件格式（表6-2列出了一些）。比如说，你可以用skiprows跳过文件的第一行、第三行和第四行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [23]: !cat examples/ex4.csv\n",
    "# hey!\n",
    "a,b,c,d,message\n",
    "# just wanted to make things more difficult for you\n",
    "# who reads CSV files with computers, anyway?\n",
    "1,2,3,4,hello\n",
    "5,6,7,8,world\n",
    "9,10,11,12,foo\n",
    "In [24]: pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3])\n",
    "Out[24]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92840bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "缺失值处理是文件解析任务中的一个重要组成部分。缺失数据经常是要么没有（空字符串），要么用某个标记值表示。默认情况下，pandas会用一组经常出现的标记值进行识别，比如NA及NULL："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb97409",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [25]: !cat examples/ex5.csv\n",
    "something,a,b,c,d,message\n",
    "one,1,2,3,4,NA\n",
    "two,5,6,,8,world\n",
    "three,9,10,11,12,foo\n",
    "In [26]: result = pd.read_csv('examples/ex5.csv')\n",
    "\n",
    "In [27]: result\n",
    "Out[27]: \n",
    "  something  a   b     c   d message\n",
    "0       one  1   2   3.0   4     NaN\n",
    "1       two  5   6   NaN   8   world\n",
    "2     three  9  10  11.0  12     foo\n",
    "\n",
    "In [28]: pd.isnull(result)\n",
    "Out[28]: \n",
    "   something      a      b      c      d  message\n",
    "0      False  False  False  False  False     True\n",
    "1      False  False  False   True  False    False\n",
    "2      False  False  False  False  False    False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca1982",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "na_values可以用一个列表或集合的字符串表示缺失值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [29]: result = pd.read_csv('examples/ex5.csv', na_values=['NULL'])\n",
    "\n",
    "In [30]: result\n",
    "Out[30]: \n",
    "  something  a   b     c   d message\n",
    "0       one  1   2   3.0   4     NaN\n",
    "1       two  5   6   NaN   8   world\n",
    "2     three  9  10  11.0  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e807f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "字典的各列可以使用不同的NA标记值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [31]: sentinels = {'message': ['foo', 'NA'], 'something': ['two']}\n",
    "\n",
    "In [32]: pd.read_csv('examples/ex5.csv', na_values=sentinels)\n",
    "Out[32]:\n",
    "something  a   b     c   d message\n",
    "0       one  1   2   3.0   4     NaN\n",
    "1       NaN  5   6   NaN   8   world\n",
    "2     three  9  10  11.0  12     NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef682be4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "表6-2列出了pandas.read_csv和pandas.read_table常用的选项。\n",
    "\n",
    "![](./figures/t6-2.png)\n",
    "\n",
    "![](./figures/t6-2-1.png)\n",
    "\n",
    "![](./figures/t6-2-2.png)\n",
    "\n",
    "## 逐块读取文本文件\n",
    "在处理很大的文件时，或找出大文件中的参数集以便于后续处理时，你可能只想读取文件的一小部分或逐块对文件进行迭代。\n",
    "\n",
    "在看大文件之前，我们先设置pandas显示地更紧些："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d47d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [33]: pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4ddbf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a42edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [34]: result = pd.read_csv('examples/ex6.csv')\n",
    "\n",
    "In [35]: result\n",
    "Out[35]: \n",
    "           one       two     three      four key\n",
    "0     0.467976 -0.038649 -0.295344 -1.824726   L\n",
    "1    -0.358893  1.404453  0.704965 -0.200638   B\n",
    "2    -0.501840  0.659254 -0.421691 -0.057688   G\n",
    "3     0.204886  1.074134  1.388361 -0.982404   R\n",
    "4     0.354628 -0.133116  0.283763 -0.837063   Q\n",
    "...        ...       ...       ...       ...  ..\n",
    "9995  2.311896 -0.417070 -1.409599 -0.515821   L\n",
    "9996 -0.479893 -0.650419  0.745152 -0.646038   E\n",
    "9997  0.523331  0.787112  0.486066  1.093156   K\n",
    "9998 -0.362559  0.598894 -1.843201  0.887292   G\n",
    "9999 -0.096376 -1.012999 -0.657431 -0.573315   0\n",
    "[10000 rows x 5 columns]\n",
    "If you want to only read a small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60fb11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果只想读取几行（避免读取整个文件），通过nrows进行指定即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86799bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [36]: pd.read_csv('examples/ex6.csv', nrows=5)\n",
    "Out[36]: \n",
    "        one       two     three      four key\n",
    "0  0.467976 -0.038649 -0.295344 -1.824726   L\n",
    "1 -0.358893  1.404453  0.704965 -0.200638   B\n",
    "2 -0.501840  0.659254 -0.421691 -0.057688   G\n",
    "3  0.204886  1.074134  1.388361 -0.982404   R\n",
    "4  0.354628 -0.133116  0.283763 -0.837063   Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd295d1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "要逐块读取文件，可以指定chunksize（行数）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [874]: chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\n",
    "\n",
    "In [875]: chunker\n",
    "Out[875]: <pandas.io.parsers.TextParser at 0x8398150>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d69094",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "read_csv所返回的这个TextParser对象使你可以根据chunksize对文件进行逐块迭代。比如说，我们可以迭代处理ex6.csv，将值计数聚合到\"key\"列中，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)\n",
    "\n",
    "tot = pd.Series([])\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "\n",
    "tot = tot.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629f26a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c50257",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [40]: tot[:10]\n",
    "Out[40]: \n",
    "E    368.0\n",
    "X    364.0\n",
    "L    346.0\n",
    "O    343.0\n",
    "Q    340.0\n",
    "M    338.0\n",
    "J    337.0\n",
    "F    335.0\n",
    "K    334.0\n",
    "H    330.0\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d348b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "TextParser还有一个get_chunk方法，它使你可以读取任意大小的块。\n",
    "\n",
    "## 将数据写出到文本格式\n",
    "数据也可以被输出为分隔符格式的文本。我们再来看看之前读过的一个CSV文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [41]: data = pd.read_csv('examples/ex5.csv')\n",
    "\n",
    "In [42]: data\n",
    "Out[42]: \n",
    "  something  a   b     c   d message\n",
    "0       one  1   2   3.0   4     NaN\n",
    "1       two  5   6   NaN   8   world\n",
    "2     three  9  10  11.0  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbafcb7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "利用DataFrame的to_csv方法，我们可以将数据写到一个以逗号分隔的文件中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea633860",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [43]: data.to_csv('examples/out.csv')\n",
    "\n",
    "In [44]: !cat examples/out.csv\n",
    ",something,a,b,c,d,message\n",
    "0,one,1,2,3.0,4,\n",
    "1,two,5,6,,8,world\n",
    "2,three,9,10,11.0,12,foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14628f4a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "当然，还可以使用其他分隔符（由于这里直接写出到sys.stdout，所以仅仅是打印出文本结果而已）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [45]: import sys\n",
    "\n",
    "In [46]: data.to_csv(sys.stdout, sep='|')\n",
    "|something|a|b|c|d|message\n",
    "0|one|1|2|3.0|4|\n",
    "1|two|5|6||8|world\n",
    "2|three|9|10|11.0|12|foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545efa7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "缺失值在输出结果中会被表示为空字符串。你可能希望将其表示为别的标记值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e622c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [47]: data.to_csv(sys.stdout, na_rep='NULL')\n",
    ",something,a,b,c,d,message\n",
    "0,one,1,2,3.0,4,NULL\n",
    "1,two,5,6,NULL,8,world\n",
    "2,three,9,10,11.0,12,foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ae4f6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果没有设置其他选项，则会写出行和列的标签。当然，它们也都可以被禁用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df044073",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [48]: data.to_csv(sys.stdout, index=False, header=False)\n",
    "one,1,2,3.0,4,\n",
    "two,5,6,,8,world\n",
    "three,9,10,11.0,12,foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e169d4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "此外，你还可以只写出一部分的列，并以你指定的顺序排列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [49]: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])\n",
    "a,b,c\n",
    "1,2,3.0\n",
    "5,6,\n",
    "9,10,11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53279c39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Series也有一个to_csv方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [50]: dates = pd.date_range('1/1/2000', periods=7)\n",
    "\n",
    "In [51]: ts = pd.Series(np.arange(7), index=dates)\n",
    "\n",
    "In [52]: ts.to_csv('examples/tseries.csv')\n",
    "\n",
    "In [53]: !cat examples/tseries.csv\n",
    "2000-01-01,0\n",
    "2000-01-02,1\n",
    "2000-01-03,2\n",
    "2000-01-04,3\n",
    "2000-01-05,4\n",
    "2000-01-06,5\n",
    "2000-01-07,6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbdf58f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 处理分隔符格式\n",
    "大部分存储在磁盘上的表格型数据都能用pandas.read_table进行加载。然而，有时还是需要做一些手工处理。由于接收到含有畸形行的文件而使read_table出毛病的情况并不少见。为了说明这些基本工具，看看下面这个简单的CSV文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [54]: !cat examples/ex7.csv\n",
    "\"a\",\"b\",\"c\"\n",
    "\"1\",\"2\",\"3\"\n",
    "\"1\",\"2\",\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d238f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "对于任何单字符分隔符文件，可以直接使用Python内置的csv模块。将任意已打开的文件或文件型的对象传给csv.reader："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('examples/ex7.csv')\n",
    "\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b6850",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "对这个reader进行迭代将会为每行产生一个元组（并移除了所有的引号）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [56]: for line in reader:\n",
    "   ....:     print(line)\n",
    "['a', 'b', 'c']\n",
    "['1', '2', '3']\n",
    "['1', '2', '3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e66544",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，为了使数据格式合乎要求，你需要对其做一些整理工作。我们一步一步来做。首先，读取文件到一个多行的列表中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [57]: with open('examples/ex7.csv') as f:\n",
    "   ....:     lines = list(csv.reader(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98776ba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后，我们将这些行分为标题行和数据行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [58]: header, values = lines[0], lines[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71a00a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后，我们可以用字典构造式和zip(*values)，后者将行转置为列，创建数据列的字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [59]: data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "\n",
    "In [60]: data_dict\n",
    "Out[60]: {'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b38f5d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "CSV文件的形式有很多。只需定义csv.Dialect的一个子类即可定义出新格式（如专门的分隔符、字符串引用约定、行结束符等）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf377a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "reader = csv.reader(f, dialect=my_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194a3c6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "各个CSV语支的参数也可以用关键字的形式提供给csv.reader，而无需定义子类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(f, delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d329d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "可用的选项（csv.Dialect的属性）及其功能如表6-3所示。\n",
    "\n",
    "![](./figures/t6-3.png)\n",
    "\n",
    ">笔记：对于那些使用复杂分隔符或多字符分隔符的文件，csv模块就无能为力了。这种情况下，你就只能使用字符串的split方法或正则表达式方法re.split进行行拆分和其他整理工作了。\n",
    "\n",
    "要手工输出分隔符文件，你可以使用csv.writer。它接受一个已打开且可写的文件对象以及跟csv.reader相同的那些语支和格式化选项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf36a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mydata.csv', 'w') as f:\n",
    "    writer = csv.writer(f, dialect=my_dialect)\n",
    "    writer.writerow(('one', 'two', 'three'))\n",
    "    writer.writerow(('1', '2', '3'))\n",
    "    writer.writerow(('4', '5', '6'))\n",
    "    writer.writerow(('7', '8', '9'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82008326",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## JSON数据\n",
    "JSON（JavaScript Object Notation的简称）已经成为通过HTTP请求在Web浏览器和其他应用程序之间发送数据的标准格式之一。它是一种比表格型文本格式（如CSV）灵活得多的数据格式。下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551d02f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 38,\n",
    "               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4810c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "除其空值null和一些其他的细微差别（如列表末尾不允许存在多余的逗号）之外，JSON非常接近于有效的Python代码。基本类型有对象（字典）、数组（列表）、字符串、数值、布尔值以及null。对象中所有的键都必须是字符串。许多Python库都可以读写JSON数据。我将使用json，因为它是构建于Python标准库中的。通过json.loads即可将JSON字符串转换成Python形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [62]: import json\n",
    "\n",
    "In [63]: result = json.loads(obj)\n",
    "\n",
    "In [64]: result\n",
    "Out[64]: \n",
    "{'name': 'Wes',\n",
    " 'pet': None,\n",
    " 'places_lived': ['United States', 'Spain', 'Germany'],\n",
    " 'siblings': [{'age': 30, 'name': 'Scott', 'pets': ['Zeus', 'Zuko']},\n",
    "  {'age': 38, 'name': 'Katie', 'pets': ['Sixes', 'Stache', 'Cisco']}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe09216",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "json.dumps则将Python对象转换成JSON格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [65]: asjson = json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a9c2d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如何将（一个或一组）JSON对象转换为DataFrame或其他便于分析的数据结构就由你决定了。最简单方便的方式是：向DataFrame构造器传入一个字典的列表（就是原先的JSON对象），并选取数据字段的子集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c33007",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [66]: siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n",
    "\n",
    "In [67]: siblings\n",
    "Out[67]: \n",
    "    name  age\n",
    "0  Scott   30\n",
    "1  Katie   38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c13339",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "pandas.read_json可以自动将特别格式的JSON数据集转换为Series或DataFrame。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [68]: !cat examples/example.json\n",
    "[{\"a\": 1, \"b\": 2, \"c\": 3},\n",
    " {\"a\": 4, \"b\": 5, \"c\": 6},\n",
    " {\"a\": 7, \"b\": 8, \"c\": 9}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f07c8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "pandas.read_json的默认选项假设JSON数组中的每个对象是表格中的一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [69]: data = pd.read_json('examples/example.json')\n",
    "\n",
    "In [70]: data\n",
    "Out[70]: \n",
    "   a  b  c\n",
    "0  1  2  3\n",
    "1  4  5  6\n",
    "2  7  8  9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca994c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "第7章中关于USDA Food Database的那个例子进一步讲解了JSON数据的读取和处理（包括嵌套记录）。\n",
    "\n",
    "如果你需要将数据从pandas输出到JSON，可以使用to_json方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [71]: print(data.to_json())\n",
    "{\"a\":{\"0\":1,\"1\":4,\"2\":7},\"b\":{\"0\":2,\"1\":5,\"2\":8},\"c\":{\"0\":3,\"1\":6,\"2\":9}}\n",
    "\n",
    "In [72]: print(data.to_json(orient='records'))\n",
    "[{\"a\":1,\"b\":2,\"c\":3},{\"a\":4,\"b\":5,\"c\":6},{\"a\":7,\"b\":8,\"c\":9}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8882d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## XML和HTML：Web信息收集\n",
    "\n",
    "Python有许多可以读写常见的HTML和XML格式数据的库，包括lxml、Beautiful Soup和html5lib。lxml的速度比较快，但其它的库处理有误的HTML或XML文件更好。\n",
    "\n",
    "pandas有一个内置的功能，read_html，它可以使用lxml和Beautiful Soup自动将HTML文件中的表格解析为DataFrame对象。为了进行展示，我从美国联邦存款保险公司下载了一个HTML文件（pandas文档中也使用过），它记录了银行倒闭的情况。首先，你需要安装read_html用到的库：\n",
    "```\n",
    "conda install lxml\n",
    "pip install beautifulsoup4 html5lib\n",
    "```\n",
    "\n",
    "如果你用的不是conda，可以使用``pip install lxml``。\n",
    "\n",
    "pandas.read_html有一些选项，默认条件下，它会搜索、尝试解析<table>标签内的的表格数据。结果是一个列表的DataFrame对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216918da",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [73]: tables = pd.read_html('examples/fdic_failed_bank_list.html')\n",
    "\n",
    "In [74]: len(tables)\n",
    "Out[74]: 1\n",
    "\n",
    "In [75]: failures = tables[0]\n",
    "\n",
    "In [76]: failures.head()\n",
    "Out[76]: \n",
    "                      Bank Name             City  ST   CERT  \\\n",
    "0                   Allied Bank         Mulberry  AR     91   \n",
    "1  The Woodbury Banking Company         Woodbury  GA  11297   \n",
    "2        First CornerStone Bank  King of Prussia  PA  35312   \n",
    "3            Trust Company Bank          Memphis  TN   9956   \n",
    "4    North Milwaukee State Bank        Milwaukee  WI  20364   \n",
    "                 Acquiring Institution        Closing Date       Updated Date  \n",
    "0                         Today's Bank  September 23, 2016  November 17, 2016  \n",
    "1                          United Bank     August 19, 2016  November 17, 2016  \n",
    "2  First-Citizens Bank & Trust Company         May 6, 2016  September 6, 2016  \n",
    "3           The Bank of Fayette County      April 29, 2016  September 6, 2016  \n",
    "4  First-Citizens Bank & Trust Company      March 11, 2016      June 16, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b886d8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "因为failures有许多列，pandas插入了一个换行符\\。\n",
    "\n",
    "这里，我们可以做一些数据清洗和分析（后面章节会进一步讲解），比如计算按年份计算倒闭的银行数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [77]: close_timestamps = pd.to_datetime(failures['Closing Date'])\n",
    "\n",
    "In [78]: close_timestamps.dt.year.value_counts()\n",
    "Out[78]: \n",
    "2010    157\n",
    "2009    140\n",
    "2011     92\n",
    "2012     51\n",
    "2008     25\n",
    "       ... \n",
    "2004      4\n",
    "2001      4\n",
    "2007      3\n",
    "2003      3\n",
    "2000      2\n",
    "Name: Closing Date, Length: 15, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a3546",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 利用lxml.objectify解析XML\n",
    "XML（Extensible Markup Language）是另一种常见的支持分层、嵌套数据以及元数据的结构化数据格式。本书所使用的这些文件实际上来自于一个很大的XML文档。\n",
    "\n",
    "前面，我介绍了pandas.read_html函数，它可以使用lxml或Beautiful Soup从HTML解析数据。XML和HTML的结构很相似，但XML更为通用。这里，我会用一个例子演示如何利用lxml从XML格式解析数据。\n",
    "\n",
    "纽约大都会运输署发布了一些有关其公交和列车服务的数据资料（http://www.mta.info/developers/download.html）。这里，我们将看看包含在一组XML文件中的运行情况数据。每项列车或公交服务都有各自的文件（如Metro-North Railroad的文件是Performance_MNR.xml），其中每条XML记录就是一条月度数据，如下所示：\n",
    "```xml\n",
    "<INDICATOR>\n",
    "  <INDICATOR_SEQ>373889</INDICATOR_SEQ>\n",
    "  <PARENT_SEQ></PARENT_SEQ>\n",
    "  <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>\n",
    "  <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>\n",
    "  <DESCRIPTION>Percent of the time that escalators are operational\n",
    "  systemwide. The availability rate is based on physical observations performed\n",
    "  the morning of regular business days only. This is a new indicator the agency\n",
    "  began reporting in 2009.</DESCRIPTION>\n",
    "  <PERIOD_YEAR>2011</PERIOD_YEAR>\n",
    "  <PERIOD_MONTH>12</PERIOD_MONTH>\n",
    "  <CATEGORY>Service Indicators</CATEGORY>\n",
    "  <FREQUENCY>M</FREQUENCY>\n",
    "  <DESIRED_CHANGE>U</DESIRED_CHANGE>\n",
    "  <INDICATOR_UNIT>%</INDICATOR_UNIT>\n",
    "  <DECIMAL_PLACES>1</DECIMAL_PLACES>\n",
    "  <YTD_TARGET>97.00</YTD_TARGET>\n",
    "  <YTD_ACTUAL></YTD_ACTUAL>\n",
    "  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>\n",
    "  <MONTHLY_ACTUAL></MONTHLY_ACTUAL>\n",
    "</INDICATOR>\n",
    "```\n",
    "\n",
    "我们先用lxml.objectify解析该文件，然后通过getroot得到该XML文件的根节点的引用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa27efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "\n",
    "path = 'datasets/mta_perf/Performance_MNR.xml'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7a86c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "root.INDICATOR返回一个用于产生各个<INDICATOR>XML元素的生成器。对于每条记录，我们可以用标记名（如YTD_ACTUAL）和数据值填充一个字典（排除几个标记）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\n",
    "               'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
    "\n",
    "for elt in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e124cb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "最后，将这组字典转换为一个DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38159c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [81]: perf = pd.DataFrame(data)\n",
    "\n",
    "In [82]: perf.head()\n",
    "Out[82]:\n",
    "Empty DataFrame\n",
    "Columns: []\n",
    "Index: []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2fd61",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "XML数据可以比本例复杂得多。每个标记都可以有元数据。看看下面这个HTML的链接标签（它也算是一段有效的XML）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3372ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "tag = '<a href=\"http://www.baidu.com\">Baidu</a>'\n",
    "root = objectify.parse(StringIO(tag)).getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc036f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在就可以访问标签或链接文本中的任何字段了（如href）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52a1553",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroot\u001b[49m\n\u001b[1;32m      3\u001b[0m root\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m root\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "root\n",
    "\n",
    "root.get('href')\n",
    "\n",
    "\n",
    "root.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d6440",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 6.2 二进制数据格式\n",
    "\n",
    "实现数据的高效二进制格式存储最简单的办法之一是使用Python内置的pickle序列化。pandas对象都有一个用于将数据以pickle格式保存到磁盘上的to_pickle方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [87]: frame = pd.read_csv('examples/ex1.csv')\n",
    "\n",
    "In [88]: frame\n",
    "Out[88]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo\n",
    "\n",
    "In [89]: frame.to_pickle('examples/frame_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f5ad0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "你可以通过pickle直接读取被pickle化的数据，或是使用更为方便的pandas.read_pickle："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cded0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [90]: pd.read_pickle('examples/frame_pickle')\n",
    "Out[90]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9a835",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    ">注意：pickle仅建议用于短期存储格式。其原因是很难保证该格式永远是稳定的；今天pickle的对象可能无法被后续版本的库unpickle出来。虽然我尽力保证这种事情不会发生在pandas中，但是今后的某个时候说不定还是得“打破”该pickle格式。\n",
    "\n",
    "pandas内置支持两个二进制数据格式：HDF5和MessagePack。下一节，我会给出几个HDF5的例子，但我建议你尝试下不同的文件格式，看看它们的速度以及是否适合你的分析工作。pandas或NumPy数据的其它存储格式有：\n",
    "\n",
    "- bcolz：一种可压缩的列存储二进制格式，基于Blosc压缩库。\n",
    "- Feather：我与R语言社区的Hadley Wickham设计的一种跨语言的列存储文件格式。Feather使用了Apache Arrow的列式内存格式。\n",
    "\n",
    "## 使用HDF5格式\n",
    "\n",
    "HDF5是一种存储大规模科学数组数据的非常好的文件格式。它可以被作为C标准库，带有许多语言的接口，如Java、Python和MATLAB等。HDF5中的HDF指的是层次型数据格式（hierarchical data format）。每个HDF5文件都含有一个文件系统式的节点结构，它使你能够存储多个数据集并支持元数据。与其他简单格式相比，HDF5支持多种压缩器的即时压缩，还能更高效地存储重复模式数据。对于那些非常大的无法直接放入内存的数据集，HDF5就是不错的选择，因为它可以高效地分块读写。\n",
    "\n",
    "虽然可以用PyTables或h5py库直接访问HDF5文件，pandas提供了更为高级的接口，可以简化存储Series和DataFrame对象。HDFStore类可以像字典一样，处理低级的细节："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567907ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [92]: frame = pd.DataFrame({'a': np.random.randn(100)})\n",
    "\n",
    "In [93]: store = pd.HDFStore('mydata.h5')\n",
    "\n",
    "In [94]: store['obj1'] = frame\n",
    "\n",
    "In [95]: store['obj1_col'] = frame['a']\n",
    "\n",
    "In [96]: store\n",
    "Out[96]: \n",
    "<class 'pandas.io.pytables.HDFStore'>\n",
    "File path: mydata.h5\n",
    "/obj1                frame        (shape->[100,1])                               \n",
    "        \n",
    "/obj1_col            series       (shape->[100])                                 \n",
    "        \n",
    "/obj2                frame_table  (typ->appendable,nrows->100,ncols->1,indexers->\n",
    "[index])\n",
    "/obj3                frame_table  (typ->appendable,nrows->100,ncols->1,indexers->\n",
    "[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42348a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "HDF5文件中的对象可以通过与字典一样的API进行获取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecea807",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [97]: store['obj1']\n",
    "Out[97]: \n",
    "           a\n",
    "0  -0.204708\n",
    "1   0.478943\n",
    "2  -0.519439\n",
    "3  -0.555730\n",
    "4   1.965781\n",
    "..       ...\n",
    "95  0.795253\n",
    "96  0.118110\n",
    "97 -0.748532\n",
    "98  0.584970\n",
    "99  0.152677\n",
    "[100 rows x 1 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f7f5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "HDFStore支持两种存储模式，'fixed'和'table'。后者通常会更慢，但是支持使用特殊语法进行查询操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47617d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [98]: store.put('obj2', frame, format='table')\n",
    "\n",
    "In [99]: store.select('obj2', where=['index >= 10 and index <= 15'])\n",
    "Out[99]: \n",
    "           a\n",
    "10  1.007189\n",
    "11 -1.296221\n",
    "12  0.274992\n",
    "13  0.228913\n",
    "14  1.352917\n",
    "15  0.886429\n",
    "\n",
    "In [100]: store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748b477",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "put是store['obj2'] = frame方法的显示版本，允许我们设置其它的选项，比如格式。\n",
    "\n",
    "pandas.read_hdf函数可以快捷使用这些工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b87111",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [101]: frame.to_hdf('mydata.h5', 'obj3', format='table')\n",
    "\n",
    "In [102]: pd.read_hdf('mydata.h5', 'obj3', where=['index < 5'])\n",
    "Out[102]: \n",
    "          a\n",
    "0 -0.204708\n",
    "1  0.478943\n",
    "2 -0.519439\n",
    "3 -0.555730\n",
    "4  1.965781"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a8663",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    ">笔记：如果你要处理的数据位于远程服务器，比如Amazon S3或HDFS，使用专门为分布式存储（比如Apache Parquet）的二进制格式也许更加合适。Python的Parquet和其它存储格式还在不断的发展之中，所以这本书中没有涉及。\n",
    "\n",
    "如果需要本地处理海量数据，我建议你好好研究一下PyTables和h5py，看看它们能满足你的哪些需求。。由于许多数据分析问题都是IO密集型（而不是CPU密集型），利用HDF5这样的工具能显著提升应用程序的效率。\n",
    "\n",
    ">注意：HDF5不是数据库。它最适合用作“一次写多次读”的数据集。虽然数据可以在任何时候被添加到文件中，但如果同时发生多个写操作，文件就可能会被破坏。\n",
    "\n",
    "## 读取Microsoft Excel文件\n",
    "\n",
    "pandas的ExcelFile类或pandas.read_excel函数支持读取存储在Excel 2003（或更高版本）中的表格型数据。这两个工具分别使用扩展包xlrd和openpyxl读取XLS和XLSX文件。你可以用pip或conda安装它们。\n",
    "\n",
    "要使用ExcelFile，通过传递xls或xlsx路径创建一个实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3261b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [104]: xlsx = pd.ExcelFile('examples/ex1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae94cf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "存储在表单中的数据可以read_excel读取到DataFrame（原书这里写的是用parse解析，但代码中用的是read_excel，是个笔误：只换了代码，没有改文字）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [105]: pd.read_excel(xlsx, 'Sheet1')\n",
    "Out[105]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ff93e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果要读取一个文件中的多个表单，创建ExcelFile会更快，但你也可以将文件名传递到pandas.read_excel："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [106]: frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1')\n",
    "\n",
    "In [107]: frame\n",
    "Out[107]: \n",
    "   a   b   c   d message\n",
    "0  1   2   3   4   hello\n",
    "1  5   6   7   8   world\n",
    "2  9  10  11  12     foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf53b9a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果要将pandas数据写入为Excel格式，你必须首先创建一个ExcelWriter，然后使用pandas对象的to_excel方法将数据写入到其中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ce97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [108]: writer = pd.ExcelWriter('examples/ex2.xlsx')\n",
    "\n",
    "In [109]: frame.to_excel(writer, 'Sheet1')\n",
    "\n",
    "In [110]: writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c0fba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "你还可以不使用ExcelWriter，而是传递文件的路径到to_excel："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [111]: frame.to_excel('examples/ex2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880d920",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 6.3 Web APIs交互\n",
    "许多网站都有一些通过JSON或其他格式提供数据的公共API。通过Python访问这些API的办法有不少。一个简单易用的办法（推荐）是requests包（http://docs.python-requests.org）。\n",
    "\n",
    "为了搜索最新的30个GitHub上的pandas主题，我们可以发一个HTTP GET请求，使用requests扩展库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [113]: import requests\n",
    "\n",
    "In [114]: url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "\n",
    "In [115]: resp = requests.get(url)\n",
    "\n",
    "In [116]: resp\n",
    "Out[116]: <Response [200]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fd12a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "响应对象的json方法会返回一个包含被解析过的JSON字典，加载到一个Python对象中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [117]: data = resp.json()\n",
    "\n",
    "In [118]: data[0]['title']\n",
    "Out[118]: 'Period does not round down for frequencies less that 1 hour'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2204fc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "data中的每个元素都是一个包含所有GitHub主题页数据（不包含评论）的字典。我们可以直接传递数据到DataFrame，并提取感兴趣的字段："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c159f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [119]: issues = pd.DataFrame(data, columns=['number', 'title',\n",
    "   .....:                                      'labels', 'state'])\n",
    "\n",
    "In [120]: issues\n",
    "Out[120]:\n",
    "    number                                              title  \\\n",
    "0    17666  Period does not round down for frequencies les...   \n",
    "1    17665           DOC: improve docstring of function where   \n",
    "2    17664               COMPAT: skip 32-bit test on int repr   \n",
    "3    17662                          implement Delegator class\n",
    "4    17654  BUG: Fix series rename called with str alterin...   \n",
    "..     ...                                                ...   \n",
    "25   17603  BUG: Correctly localize naive datetime strings...   \n",
    "26   17599                     core.dtypes.generic --> cython   \n",
    "27   17596   Merge cdate_range functionality into bdate_range   \n",
    "28   17587  Time Grouper bug fix when applied for list gro...   \n",
    "29   17583  BUG: fix tz-aware DatetimeIndex + TimedeltaInd...   \n",
    "                                               labels state  \n",
    "0                                                  []  open  \n",
    "1   [{'id': 134699, 'url': 'https://api.github.com...  open  \n",
    "2   [{'id': 563047854, 'url': 'https://api.github....  open  \n",
    "3                                                  []  open  \n",
    "4   [{'id': 76811, 'url': 'https://api.github.com/...  open  \n",
    "..                                                ...   ...  \n",
    "25  [{'id': 76811, 'url': 'https://api.github.com/...  open  \n",
    "26  [{'id': 49094459, 'url': 'https://api.github.c...  open  \n",
    "27  [{'id': 35818298, 'url': 'https://api.github.c...  open  \n",
    "28  [{'id': 233160, 'url': 'https://api.github.com...  open  \n",
    "29  [{'id': 76811, 'url': 'https://api.github.com/...  open  \n",
    "[30 rows x 4 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b894b49",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "花费一些精力，你就可以创建一些更高级的常见的Web API的接口，返回DataFrame对象，方便进行分析。\n",
    "\n",
    "# 6.4 数据库交互\n",
    "\n",
    "在商业场景下，大多数数据可能不是存储在文本或Excel文件中。基于SQL的关系型数据库（如SQL Server、PostgreSQL和MySQL等）使用非常广泛，其它一些数据库也很流行。数据库的选择通常取决于性能、数据完整性以及应用程序的伸缩性需求。\n",
    "\n",
    "将数据从SQL加载到DataFrame的过程很简单，此外pandas还有一些能够简化该过程的函数。例如，我将使用SQLite数据库（通过Python内置的sqlite3驱动器）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [121]: import sqlite3\n",
    "\n",
    "In [122]: query = \"\"\"\n",
    "   .....: CREATE TABLE test\n",
    "   .....: (a VARCHAR(20), b VARCHAR(20),\n",
    "   .....:  c REAL,        d INTEGER\n",
    "   .....: );\"\"\"\n",
    "\n",
    "In [123]: con = sqlite3.connect('mydata.sqlite')\n",
    "\n",
    "In [124]: con.execute(query)\n",
    "Out[124]: <sqlite3.Cursor at 0x7f6b12a50f10>\n",
    "\n",
    "In [125]: con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11838f85",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后插入几行数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [126]: data = [('Atlanta', 'Georgia', 1.25, 6),\n",
    "   .....:         ('Tallahassee', 'Florida', 2.6, 3),\n",
    "   .....:         ('Sacramento', 'California', 1.7, 5)]\n",
    "\n",
    "In [127]: stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\n",
    "\n",
    "In [128]: con.executemany(stmt, data)\n",
    "Out[128]: <sqlite3.Cursor at 0x7f6b15c66ce0>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2234b23",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "从表中选取数据时，大部分Python SQL驱动器（PyODBC、psycopg2、MySQLdb、pymssql等）都会返回一个元组列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [130]: cursor = con.execute('select * from test')\n",
    "\n",
    "In [131]: rows = cursor.fetchall()\n",
    "\n",
    "In [132]: rows\n",
    "Out[132]: \n",
    "[('Atlanta', 'Georgia', 1.25, 6),\n",
    " ('Tallahassee', 'Florida', 2.6, 3),\n",
    " ('Sacramento', 'California', 1.7, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77af98b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "你可以将这个元组列表传给DataFrame构造器，但还需要列名（位于光标的description属性中）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2367b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [133]: cursor.description\n",
    "Out[133]: \n",
    "(('a', None, None, None, None, None, None),\n",
    " ('b', None, None, None, None, None, None),\n",
    " ('c', None, None, None, None, None, None),\n",
    " ('d', None, None, None, None, None, None))\n",
    "\n",
    "In [134]: pd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n",
    "Out[134]: \n",
    "             a           b     c  d\n",
    "0      Atlanta     Georgia  1.25  6\n",
    "1  Tallahassee     Florida  2.60  3\n",
    "2   Sacramento  California  1.70  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea5ecc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这种数据规整操作相当多，你肯定不想每查一次数据库就重写一次。[SQLAlchemy项目](http://www.sqlalchemy.org/)是一个流行的Python SQL工具，它抽象出了SQL数据库中的许多常见差异。pandas有一个read_sql函数，可以让你轻松的从SQLAlchemy连接读取数据。这里，我们用SQLAlchemy连接SQLite数据库，并从之前创建的表读取数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [135]: import sqlalchemy as sqla\n",
    "\n",
    "In [136]: db = sqla.create_engine('sqlite:///mydata.sqlite')\n",
    "\n",
    "In [137]: pd.read_sql('select * from test', db)\n",
    "Out[137]: \n",
    "             a           b     c  d\n",
    "0      Atlanta     Georgia  1.25  6\n",
    "1  Tallahassee     Florida  2.60  3\n",
    "2   Sacramento  California  1.70  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b609db",
   "metadata": {},
   "source": [
    "# 6.5 总结\n",
    "\n",
    "访问数据通常是数据分析的第一步。在本章中，我们已经学了一些有用的工具。在接下来的章节中，我们将深入研究数据规整、数据可视化、时间序列分析和其它主题。 "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
