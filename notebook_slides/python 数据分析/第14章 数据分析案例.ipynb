{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a91d3b7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "本书正文的最后一章，我们来看一些真实世界的数据集。对于每个数据集，我们会用之前介绍的方法，从原始数据中提取有意义的内容。展示的方法适用于其它数据集，也包括你的。本章包含了一些各种各样的案例数据集，可以用来练习。\n",
    "\n",
    "案例数据集可以在Github仓库找到，见第一章。\n",
    "\n",
    "#14.1 来自Bitly的USA.gov数据\n",
    "\n",
    "2011年，URL缩短服务Bitly跟美国政府网站USA.gov合作，提供了一份从生成.gov或.mil短链接的用户那里收集来的匿名数据。在2011年，除实时数据之外，还可以下载文本文件形式的每小时快照。写作此书时（2017年），这项服务已经关闭，但我们保存一份数据用于本书的案例。\n",
    "\n",
    "以每小时快照为例，文件中各行的格式为JSON（即JavaScript Object Notation，这是一种常用的Web数据格式）。例如，如果我们只读取某个文件中的第一行，那么所看到的结果应该是下面这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [5]: path = 'datasets/bitly_usagov/example.txt'\n",
    "\n",
    "In [6]: open(path).readline()\n",
    "Out[6]: '{ \"a\": \"Mozilla\\\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\\\/535.11\n",
    "(KHTML, like Gecko) Chrome\\\\/17.0.963.78 Safari\\\\/535.11\", \"c\": \"US\", \"nk\": 1,\n",
    "\"tz\": \"America\\\\/New_York\", \"gr\": \"MA\", \"g\": \"A6qOVH\", \"h\": \"wfLQtf\", \"l\":\n",
    "\"orofrog\", \"al\": \"en-US,en;q=0.8\", \"hh\": \"1.usa.gov\", \"r\":\n",
    "\"http:\\\\/\\\\/www.facebook.com\\\\/l\\\\/7AQEFzjSi\\\\/1.usa.gov\\\\/wfLQtf\", \"u\":\n",
    "\"http:\\\\/\\\\/www.ncbi.nlm.nih.gov\\\\/pubmed\\\\/22415991\", \"t\": 1331923247, \"hc\":\n",
    "1331822918, \"cy\": \"Danvers\", \"ll\": [ 42.576698, -70.954903 ] }\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d1d6c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Python有内置或第三方模块可以将JSON字符串转换成Python字典对象。这里，我将使用json模块及其loads函数逐行加载已经下载好的数据文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116493f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = 'datasets/bitly_usagov/example.txt'\n",
    "records = [json.loads(line) for line in open(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff3d8d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，records对象就成为一组Python字典了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f500b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [18]: records[0]\n",
    "Out[18]:\n",
    "{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko)\n",
    "Chrome/17.0.963.78 Safari/535.11',\n",
    " 'al': 'en-US,en;q=0.8',\n",
    " 'c': 'US',\n",
    " 'cy': 'Danvers',\n",
    " 'g': 'A6qOVH',\n",
    " 'gr': 'MA',\n",
    " 'h': 'wfLQtf',\n",
    " 'hc': 1331822918,\n",
    " 'hh': '1.usa.gov',\n",
    " 'l': 'orofrog',\n",
    " 'll': [42.576698, -70.954903],\n",
    " 'nk': 1,\n",
    " 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',\n",
    " 't': 1331923247,\n",
    " 'tz': 'America/New_York',\n",
    " 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902045dd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##用纯Python代码对时区进行计数\n",
    "\n",
    "假设我们想要知道该数据集中最常出现的是哪个时区（即tz字段），得到答案的办法有很多。首先，我们用列表推导式取出一组时区："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a443ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [12]: time_zones = [rec['tz'] for rec in records]\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<ipython-input-12-db4fbd348da9> in <module>()\n",
    "----> 1 time_zones = [rec['tz'] for rec in records]\n",
    "<ipython-input-12-db4fbd348da9> in <listcomp>(.0)\n",
    "----> 1 time_zones = [rec['tz'] for rec in records]\n",
    "KeyError: 'tz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940757e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "晕！原来并不是所有记录都有时区字段。这个好办，只需在列表推导式末尾加上一个if 'tz'in rec判断即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bdf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [13]: time_zones = [rec['tz'] for rec in records if 'tz' in rec]\n",
    "\n",
    "In [14]: time_zones[:10]\n",
    "Out[14]: \n",
    "['America/New_York',\n",
    " 'America/Denver',\n",
    " 'America/New_York',\n",
    " 'America/Sao_Paulo',\n",
    " 'America/New_York',\n",
    " 'America/New_York',\n",
    " 'Europe/Warsaw',\n",
    " '',\n",
    " '',\n",
    " '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4086e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "只看前10个时区，我们发现有些是未知的（即空的）。虽然可以将它们过滤掉，但现在暂时先留着。接下来，为了对时区进行计数，这里介绍两个办法：一个较难（只使用标准Python库），另一个较简单（使用pandas）。计数的办法之一是在遍历时区的过程中将计数值保存在字典中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(sequence):\n",
    "    counts = {}\n",
    "    for x in sequence:\n",
    "        if x in counts:\n",
    "            counts[x] += 1\n",
    "        else:\n",
    "            counts[x] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a197c0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果使用Python标准库的更高级工具，那么你可能会将代码写得更简洁一些："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_counts2(sequence):\n",
    "    counts = defaultdict(int) # values will initialize to 0\n",
    "    for x in sequence:\n",
    "        counts[x] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b58a4d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "我将逻辑写到函数中是为了获得更高的复用性。要用它对时区进行处理，只需将time_zones传入即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [17]: counts = get_counts(time_zones)\n",
    "\n",
    "In [18]: counts['America/New_York']\n",
    "Out[18]: 1251\n",
    "\n",
    "In [19]: len(time_zones)\n",
    "Out[19]: 3440"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675994b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果想要得到前10位的时区及其计数值，我们需要用到一些有关字典的处理技巧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_counts(count_dict, n=10):\n",
    "    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]\n",
    "    value_key_pairs.sort()\n",
    "    return value_key_pairs[-n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e74e3a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [21]: top_counts(counts)\n",
    "Out[21]: \n",
    "[(33, 'America/Sao_Paulo'),\n",
    " (35, 'Europe/Madrid'),\n",
    "(36, 'Pacific/Honolulu'),\n",
    " (37, 'Asia/Tokyo'),\n",
    " (74, 'Europe/London'),\n",
    " (191, 'America/Denver'),\n",
    " (382, 'America/Los_Angeles'),\n",
    " (400, 'America/Chicago'),\n",
    " (521, ''),\n",
    " (1251, 'America/New_York')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7feeeb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果你搜索Python的标准库，你能找到collections.Counter类，它可以使这项工作更简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8124076",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [22]: from collections import Counter\n",
    "\n",
    "In [23]: counts = Counter(time_zones)\n",
    "\n",
    "In [24]: counts.most_common(10)\n",
    "Out[24]: \n",
    "[('America/New_York', 1251),\n",
    " ('', 521),\n",
    " ('America/Chicago', 400),\n",
    " ('America/Los_Angeles', 382),\n",
    " ('America/Denver', 191),\n",
    " ('Europe/London', 74),\n",
    " ('Asia/Tokyo', 37),\n",
    " ('Pacific/Honolulu', 36),\n",
    " ('Europe/Madrid', 35),\n",
    " ('America/Sao_Paulo', 33)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52f824",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 用pandas对时区进行计数\n",
    "\n",
    "从原始记录的集合创建DateFrame，与将记录列表传递到pandas.DataFrame一样简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [25]: import pandas as pd\n",
    "\n",
    "In [26]: frame = pd.DataFrame(records)\n",
    "\n",
    "In [27]: frame.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 3560 entries, 0 to 3559\n",
    "Data columns (total 18 columns):\n",
    "_heartbeat_    120 non-null float64\n",
    "a              3440 non-null object\n",
    "al             3094 non-null object\n",
    "c              2919 non-null object\n",
    "cy             2919 non-null object\n",
    "g              3440 non-null object\n",
    "gr             2919 non-null object\n",
    "h              3440 non-null object\n",
    "hc             3440 non-null float64\n",
    "hh             3440 non-null object\n",
    "kw             93 non-null object\n",
    "l              3440 non-null object\n",
    "ll             2919 non-null object\n",
    "nk             3440 non-null float64\n",
    "r              3440 non-null object\n",
    "t              3440 non-null float64\n",
    "tz             3440 non-null object\n",
    "u              3440 non-null object\n",
    "dtypes: float64(4), object(14)\n",
    "memory usage: 500.7+ KB\n",
    "\n",
    "In [28]: frame['tz'][:10]\n",
    "Out[28]: \n",
    "0     America/New_York\n",
    "1       America/Denver\n",
    "2     America/New_York\n",
    "3    America/Sao_Paulo\n",
    "4     America/New_York\n",
    "5     America/New_York\n",
    "6        Europe/Warsaw\n",
    "7                     \n",
    "8                     \n",
    "9                     \n",
    "Name: tz, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4dff7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这里frame的输出形式是摘要视图（summary view），主要用于较大的DataFrame对象。我们然后可以对Series使用value_counts方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [29]: tz_counts = frame['tz'].value_counts()\n",
    "\n",
    "In [30]: tz_counts[:10]\n",
    "Out[30]: \n",
    "America/New_York       1251\n",
    "                        521\n",
    "America/Chicago         400\n",
    "America/Los_Angeles     382\n",
    "America/Denver          191\n",
    "Europe/London            74\n",
    "Asia/Tokyo               37\n",
    "Pacific/Honolulu         36\n",
    "Europe/Madrid            35\n",
    "America/Sao_Paulo        33\n",
    "Name: tz, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3275e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "我们可以用matplotlib可视化这个数据。为此，我们先给记录中未知或缺失的时区填上一个替代值。fillna函数可以替换缺失值（NA），而未知值（空字符串）则可以通过布尔型数组索引加以替换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [31]: clean_tz = frame['tz'].fillna('Missing')\n",
    "\n",
    "In [32]: clean_tz[clean_tz == ''] = 'Unknown'\n",
    "\n",
    "In [33]: tz_counts = clean_tz.value_counts()\n",
    "\n",
    "In [34]: tz_counts[:10]\n",
    "Out[34]: \n",
    "America/New_York       1251\n",
    "Unknown                 521\n",
    "America/Chicago         400\n",
    "America/Los_Angeles     382\n",
    "America/Denver          191\n",
    "Missing                 120\n",
    "Europe/London            74\n",
    "Asia/Tokyo               37\n",
    "Pacific/Honolulu         36\n",
    "Europe/Madrid            35\n",
    "Name: tz, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b17e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "此时，我们可以用seaborn包创建水平柱状图（结果见图14-1）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [36]: import seaborn as sns\n",
    "\n",
    "In [37]: subset = tz_counts[:10]\n",
    "\n",
    "In [38]: sns.barplot(y=subset.index, x=subset.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992cd12f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-1 usa.gov示例数据中最常出现的时区](http://upload-images.jianshu.io/upload_images/7178691-aa267c1d399a78f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "a字段含有执行URL短缩操作的浏览器、设备、应用程序的相关信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5621d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [39]: frame['a'][1]\n",
    "Out[39]: 'GoogleMaps/RochesterNY'\n",
    "\n",
    "In [40]: frame['a'][50]\n",
    "Out[40]: 'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2)\n",
    "Gecko/20100101 Firefox/10.0.2'\n",
    "\n",
    "In [41]: frame['a'][51][:50]  # long line\n",
    "Out[41]: 'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P9'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694a965",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "将这些\"agent\"字符串中的所有信息都解析出来是一件挺郁闷的工作。一种策略是将这种字符串的第一节（与浏览器大致对应）分离出来并得到另外一份用户行为摘要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [42]: results = pd.Series([x.split()[0] for x in frame.a.dropna()])\n",
    "\n",
    "In [43]: results[:5]\n",
    "Out[43]: \n",
    "0               Mozilla/5.0\n",
    "1    GoogleMaps/RochesterNY\n",
    "2               Mozilla/4.0\n",
    "3               Mozilla/5.0\n",
    "4               Mozilla/5.0\n",
    "dtype: object\n",
    "\n",
    "In [44]: results.value_counts()[:8]\n",
    "Out[44]: \n",
    "Mozilla/5.0                 2594\n",
    "Mozilla/4.0                  601\n",
    "GoogleMaps/RochesterNY       121\n",
    "Opera/9.80                    34\n",
    "TEST_INTERNET_AGENT           24\n",
    "GoogleProducer                21\n",
    "Mozilla/6.0                    5\n",
    "BlackBerry8520/5.0.0.681       4\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45548fcf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，假设你想按Windows和非Windows用户对时区统计信息进行分解。为了简单起见，我们假定只要agent字符串中含有\"Windows\"就认为该用户为Windows用户。由于有的agent缺失，所以首先将它们从数据中移除："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d37acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [45]: cframe = frame[frame.a.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d940d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后计算出各行是否含有Windows的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ff9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [47]: cframe['os'] = np.where(cframe['a'].str.contains('Windows'),\n",
    "   ....:                         'Windows', 'Not Windows')\n",
    "\n",
    "In [48]: cframe['os'][:5]\n",
    "Out[48]: \n",
    "0        Windows\n",
    "1    Not Windows\n",
    "2        Windows\n",
    "3    Not Windows\n",
    "4        Windows\n",
    "Name: os, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fda29",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "接下来就可以根据时区和新得到的操作系统列表对数据进行分组了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [49]: by_tz_os = cframe.groupby(['tz', 'os'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d3ec6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "分组计数，类似于value_counts函数，可以用size来计算。并利用unstack对计数结果进行重塑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [50]: agg_counts = by_tz_os.size().unstack().fillna(0)\n",
    "\n",
    "In [51]: agg_counts[:10]\n",
    "Out[51]: \n",
    "os                              Not Windows  Windows\n",
    "tz                                                  \n",
    "                                      245.0    276.0\n",
    "Africa/Cairo                            0.0      3.0\n",
    "Africa/Casablanca                       0.0      1.0\n",
    "Africa/Ceuta                            0.0      2.0\n",
    "Africa/Johannesburg                     0.0      1.0\n",
    "Africa/Lusaka                           0.0      1.0\n",
    "America/Anchorage                       4.0      1.0\n",
    "America/Argentina/Buenos_Aires          1.0      0.0\n",
    "America/Argentina/Cordoba               0.0      1.0\n",
    "America/Argentina/Mendoza               0.0      1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bea63",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "最后，我们来选取最常出现的时区。为了达到这个目的，我根据agg_counts中的行数构造了一个间接索引数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to sort in ascending order\n",
    "In [52]: indexer = agg_counts.sum(1).argsort()\n",
    "\n",
    "In [53]: indexer[:10]\n",
    "Out[53]: \n",
    "tz\n",
    "                                  24\n",
    "Africa/Cairo                      20\n",
    "Africa/Casablanca                 21\n",
    "Africa/Ceuta                      92\n",
    "Africa/Johannesburg               87\n",
    "Africa/Lusaka                     53\n",
    "America/Anchorage                 54\n",
    "America/Argentina/Buenos_Aires    57\n",
    "America/Argentina/Cordoba         26\n",
    "America/Argentina/Mendoza         55\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfe585",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后我通过take按照这个顺序截取了最后10行最大值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49077002",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [54]: count_subset = agg_counts.take(indexer[-10:])\n",
    "\n",
    "In [55]: count_subset\n",
    "Out[55]: \n",
    "os                   Not Windows  Windows\n",
    "tz                                       \n",
    "America/Sao_Paulo           13.0     20.0\n",
    "Europe/Madrid               16.0     19.0\n",
    "Pacific/Honolulu             0.0     36.0\n",
    "Asia/Tokyo                   2.0     35.0\n",
    "Europe/London               43.0     31.0\n",
    "America/Denver             132.0     59.0\n",
    "America/Los_Angeles        130.0    252.0\n",
    "America/Chicago            115.0    285.0\n",
    "                           245.0    276.0\n",
    "America/New_York           339.0    912.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337eaed",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "pandas有一个简便方法nlargest，可以做同样的工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25eece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [56]: agg_counts.sum(1).nlargest(10)\n",
    "Out[56]: \n",
    "tz\n",
    "America/New_York       1251.0\n",
    "                        521.0\n",
    "America/Chicago         400.0\n",
    "America/Los_Angeles     382.0\n",
    "America/Denver          191.0\n",
    "Europe/London            74.0\n",
    "Asia/Tokyo               37.0\n",
    "Pacific/Honolulu         36.0\n",
    "Europe/Madrid            35.0\n",
    "America/Sao_Paulo        33.0\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b73bf5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后，如这段代码所示，可以用柱状图表示。我传递一个额外参数到seaborn的barpolt函数，来画一个堆积条形图（见图14-2）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f37a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the data for plotting\n",
    "In [58]: count_subset = count_subset.stack()\n",
    "\n",
    "In [59]: count_subset.name = 'total'\n",
    "\n",
    "In [60]: count_subset = count_subset.reset_index()\n",
    "\n",
    "In [61]: count_subset[:10]\n",
    "Out[61]: \n",
    "                  tz           os  total\n",
    "0  America/Sao_Paulo  Not Windows   13.0\n",
    "1  America/Sao_Paulo      Windows   20.0\n",
    "2      Europe/Madrid  Not Windows   16.0\n",
    "3      Europe/Madrid      Windows   19.0\n",
    "4   Pacific/Honolulu  Not Windows    0.0\n",
    "5   Pacific/Honolulu      Windows   36.0\n",
    "6         Asia/Tokyo  Not Windows    2.0\n",
    "7         Asia/Tokyo      Windows   35.0\n",
    "8      Europe/London  Not Windows   43.0\n",
    "9      Europe/London      Windows   31.0\n",
    "\n",
    "In [62]: sns.barplot(x='total', y='tz', hue='os',  data=count_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69589f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-2 最常出现时区的Windows和非Windows用户](http://upload-images.jianshu.io/upload_images/7178691-053612a5655b68d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "这张图不容易看出Windows用户在小分组中的相对比例，因此标准化分组百分比之和为1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_total(group):\n",
    "    group['normed_total'] = group.total / group.total.sum()\n",
    "    return group\n",
    "\n",
    "results = count_subset.groupby('tz').apply(norm_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d437e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "再次画图，见图14-3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [65]: sns.barplot(x='normed_total', y='tz', hue='os',  data=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90797b4a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-3 最常出现时区的Windows和非Windows用户的百分比](http://upload-images.jianshu.io/upload_images/7178691-60ee355801daf412.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "我们还可以用groupby的transform方法，更高效的计算标准化的和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [66]: g = count_subset.groupby('tz')\n",
    "\n",
    "In [67]: results2 = count_subset.total / g.total.transform('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96d90e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 14.2 MovieLens 1M数据集\n",
    "\n",
    "GroupLens Research（http://www.grouplens.org/node/73）采集了一组从20世纪90年末到21世纪初由MovieLens用户提供的电影评分数据。这些数据中包括电影评分、电影元数据（风格类型和年代）以及关于用户的人口统计学数据（年龄、邮编、性别和职业等）。基于机器学习算法的推荐系统一般都会对此类数据感兴趣。虽然我不会在本书中详细介绍机器学习技术，但我会告诉你如何对这种数据进行切片切块以满足实际需求。\n",
    "\n",
    "MovieLens 1M数据集含有来自6000名用户对4000部电影的100万条评分数据。它分为三个表：评分、用户信息和电影信息。将该数据从zip文件中解压出来之后，可以通过pandas.read_table将各个表分别读到一个pandas DataFrame对象中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make display smaller\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "users = pd.read_table('datasets/movielens/users.dat', sep='::',\n",
    "                      header=None, names=unames)\n",
    "\n",
    "rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_table('datasets/movielens/ratings.dat', sep='::',\n",
    "                        header=None, names=rnames)\n",
    "mnames = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_table('datasets/movielens/movies.dat', sep='::',\n",
    "                       header=None, names=mnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2eb63",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "利用Python的切片语法，通过查看每个DataFrame的前几行即可验证数据加载工作是否一切顺利："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d628d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [69]: users[:5]\n",
    "Out[69]: \n",
    "   user_id gender  age  occupation    zip\n",
    "0        1      F    1          10  48067\n",
    "1        2      M   56          16  70072\n",
    "2        3      M   25          15  55117\n",
    "3        4      M   45           7  02460\n",
    "4        5      M   25          20  55455\n",
    "\n",
    "In [70]: ratings[:5]\n",
    "Out[70]: \n",
    "   user_id  movie_id  rating  timestamp\n",
    "0        1      1193       5  978300760\n",
    "1        1       661       3  978302109\n",
    "2        1       914       3  978301968\n",
    "3        1      3408       4  978300275\n",
    "4        1      2355       5  978824291\n",
    "\n",
    "In [71]: movies[:5]\n",
    "Out[71]: \n",
    "   movie_id                               title                        genres\n",
    "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
    "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
    "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
    "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
    "4         5  Father of the Bride Part II (1995)                        Comedy\n",
    "\n",
    "In [72]: ratings\n",
    "Out[72]: \n",
    "         user_id  movie_id  rating  timestamp\n",
    "0              1      1193       5  978300760\n",
    "1              1       661       3  978302109\n",
    "2              1       914       3  978301968\n",
    "3              1      3408       4  978300275\n",
    "4              1      2355       5  978824291\n",
    "...          ...       ...     ...        ...\n",
    "1000204     6040      1091       1  956716541\n",
    "1000205     6040      1094       5  956704887\n",
    "1000206     6040       562       5  956704746\n",
    "1000207     6040      1096       4  956715648\n",
    "1000208     6040      1097       4  956715569\n",
    "[1000209 rows x 4 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391cc75",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "注意，其中的年龄和职业是以编码形式给出的，它们的具体含义请参考该数据集的README文件。分析散布在三个表中的数据可不是一件轻松的事情。假设我们想要根据性别和年龄计算某部电影的平均得分，如果将所有数据都合并到一个表中的话问题就简单多了。我们先用pandas的merge函数将ratings跟users合并到一起，然后再将movies也合并进去。pandas会根据列名的重叠情况推断出哪些列是合并（或连接）键："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [73]: data = pd.merge(pd.merge(ratings, users), movies)\n",
    "\n",
    "In [74]: data\n",
    "Out[74]: \n",
    "         user_id  movie_id  rating  timestamp gender  age  occupation    zip  \\\n",
    "0              1      1193       5  978300760      F    1          10  48067   \n",
    "1              2      1193       5  978298413      M   56          16  70072   \n",
    "2             12      1193       4  978220179      M   25          12  32793   \n",
    "3             15      1193       4  978199279      M   25           7  22903   \n",
    "4             17      1193       5  978158471      M   50           1  95350   \n",
    "...          ...       ...     ...        ...    ...  ...         ...    ...   \n",
    "1000204     5949      2198       5  958846401      M   18          17  47901\n",
    "1000205     5675      2703       3  976029116      M   35          14  30030   \n",
    "1000206     5780      2845       1  958153068      M   18          17  92886   \n",
    "1000207     5851      3607       5  957756608      F   18          20  55410   \n",
    "1000208     5938      2909       4  957273353      M   25           1  35401   \n",
    "                                               title                genres  \n",
    "0             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n",
    "1             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n",
    "2             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n",
    "3             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n",
    "4             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n",
    "...                                              ...                   ...  \n",
    "1000204                           Modulations (1998)           Documentary  \n",
    "1000205                        Broken Vessels (1998)                 Drama  \n",
    "1000206                            White Boys (1999)                 Drama  \n",
    "1000207                     One Little Indian (1973)  Comedy|Drama|Western  \n",
    "1000208  Five Wives, Three Secretaries and Me (1998)           Documentary  \n",
    "[1000209 rows x 10 columns]\n",
    "\n",
    "In [75]: data.iloc[0]\n",
    "Out[75]: \n",
    "user_id                                            1\n",
    "movie_id                                        1193\n",
    "rating                                             5\n",
    "timestamp                                  978300760\n",
    "gender                                             F\n",
    "age                                                1\n",
    "occupation                                        10\n",
    "zip                                            48067\n",
    "title         One Flew Over the Cuckoo's Nest (1975)\n",
    "genres                                         Drama\n",
    "Name: 0, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2d3d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "为了按性别计算每部电影的平均得分，我们可以使用pivot_table方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [76]: mean_ratings = data.pivot_table('rating', index='title',\n",
    "   ....:                                 columns='gender', aggfunc='mean')\n",
    "\n",
    "In [77]: mean_ratings[:5]\n",
    "Out[77]: \n",
    "gender                                F         M\n",
    "title                                            \n",
    "$1,000,000 Duck (1971)         3.375000  2.761905\n",
    "'Night Mother (1986)           3.388889  3.352941\n",
    "'Til There Was You (1997)      2.675676  2.733333\n",
    "'burbs, The (1989)             2.793478  2.962085\n",
    "...And Justice for All (1979)  3.828571  3.689024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a356208",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "该操作产生了另一个DataFrame，其内容为电影平均得分，行标为电影名称（索引），列标为性别。现在，我打算过滤掉评分数据不够250条的电影（随便选的一个数字）。为了达到这个目的，我先对title进行分组，然后利用size()得到一个含有各电影分组大小的Series对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebcb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [78]: ratings_by_title = data.groupby('title').size()\n",
    "\n",
    "In [79]: ratings_by_title[:10]\n",
    "Out[79]: \n",
    "title\n",
    "$1,000,000 Duck (1971)                37\n",
    "'Night Mother (1986)                  70\n",
    "'Til There Was You (1997)             52\n",
    "'burbs, The (1989)                   303\n",
    "...And Justice for All (1979)        199\n",
    "1-900 (1994)                           2\n",
    "10 Things I Hate About You (1999)    700\n",
    "101 Dalmatians (1961)                565\n",
    "101 Dalmatians (1996)                364\n",
    "12 Angry Men (1957)                  616\n",
    "dtype: int64\n",
    "\n",
    "In [80]: active_titles = ratings_by_title.index[ratings_by_title >= 250]\n",
    "\n",
    "In [81]: active_titles\n",
    "Out[81]: \n",
    "Index([''burbs, The (1989)', '10 Things I Hate About You (1999)',\n",
    "       '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)',\n",
    "       '13th Warrior, The (1999)', '2 Days in the Valley (1996)',\n",
    "       '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)',\n",
    "       '2010 (1984)',\n",
    "       ...\n",
    "'X-Men (2000)', 'Year of Living Dangerously (1982)',\n",
    "       'Yellow Submarine (1968)', 'You've Got Mail (1998)',\n",
    "       'Young Frankenstein (1974)', 'Young Guns (1988)',\n",
    "       'Young Guns II (1990)', 'Young Sherlock Holmes (1985)',\n",
    "       'Zero Effect (1998)', 'eXistenZ (1999)'],\n",
    "      dtype='object', name='title', length=1216)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153871d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "标题索引中含有评分数据大于250条的电影名称，然后我们就可以据此从前面的mean_ratings中选取所需的行了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows on the index\n",
    "In [82]: mean_ratings = mean_ratings.loc[active_titles]\n",
    "\n",
    "In [83]: mean_ratings\n",
    "Out[83]: \n",
    "gender                                    F         M\n",
    "title                                                \n",
    "'burbs, The (1989)                 2.793478  2.962085\n",
    "10 Things I Hate About You (1999)  3.646552  3.311966\n",
    "101 Dalmatians (1961)              3.791444  3.500000\n",
    "101 Dalmatians (1996)              3.240000  2.911215\n",
    "12 Angry Men (1957)                4.184397  4.328421\n",
    "...                                     ...       ...\n",
    "Young Guns (1988)                  3.371795  3.425620\n",
    "Young Guns II (1990)               2.934783  2.904025\n",
    "Young Sherlock Holmes (1985)       3.514706  3.363344\n",
    "Zero Effect (1998)                 3.864407  3.723140\n",
    "eXistenZ (1999)                    3.098592  3.289086\n",
    "[1216 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e46014",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "为了了解女性观众最喜欢的电影，我们可以对F列降序排列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5667256",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [85]: top_female_ratings = mean_ratings.sort_values(by='F', ascending=False)\n",
    "\n",
    "In [86]: top_female_ratings[:10]\n",
    "Out[86]: \n",
    "gender                                                     F         M\n",
    "title                                                                 \n",
    "Close Shave, A (1995)                               4.644444  4.473795\n",
    "Wrong Trousers, The (1993)                          4.588235  4.478261\n",
    "Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)       4.572650  4.464589\n",
    "Wallace & Gromit: The Best of Aardman Animation...  4.563107  4.385075\n",
    "Schindler's List (1993)                             4.562602  4.491415\n",
    "Shawshank Redemption, The (1994)                    4.539075  4.560625\n",
    "Grand Day Out, A (1992)                             4.537879  4.293255\n",
    "To Kill a Mockingbird (1962)                        4.536667  4.372611\n",
    "Creature Comforts (1990)                            4.513889  4.272277\n",
    "Usual Suspects, The (1995)                          4.513317  4.518248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ffadf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 计算评分分歧\n",
    "\n",
    "假设我们想要找出男性和女性观众分歧最大的电影。一个办法是给mean_ratings加上一个用于存放平均得分之差的列，并对其进行排序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb93710",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [87]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c43670",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "按\"diff\"排序即可得到分歧最大且女性观众更喜欢的电影："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea411fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [88]: sorted_by_diff = mean_ratings.sort_values(by='diff')\n",
    "\n",
    "In [89]: sorted_by_diff[:10]\n",
    "Out[89]: \n",
    "gender                                        F         M      diff\n",
    "title                                                              \n",
    "Dirty Dancing (1987)                   3.790378  2.959596 -0.830782\n",
    "Jumpin' Jack Flash (1986)              3.254717  2.578358 -0.676359\n",
    "Grease (1978)                          3.975265  3.367041 -0.608224\n",
    "Little Women (1994)                    3.870588  3.321739 -0.548849\n",
    "Steel Magnolias (1989)                 3.901734  3.365957 -0.535777\n",
    "Anastasia (1997)                       3.800000  3.281609 -0.518391\n",
    "Rocky Horror Picture Show, The (1975)  3.673016  3.160131 -0.512885\n",
    "Color Purple, The (1985)               4.158192  3.659341 -0.498851\n",
    "Age of Innocence, The (1993)           3.827068  3.339506 -0.487561\n",
    "Free Willy (1993)                      2.921348  2.438776 -0.482573"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2eec4a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "对排序结果反序并取出前10行，得到的则是男性观众更喜欢的电影："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b57033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse order of rows, take first 10 rows\n",
    "In [90]: sorted_by_diff[::-1][:10]\n",
    "Out[90]: \n",
    "gender                                         F         M      diff\n",
    "title                                                               \n",
    "Good, The Bad and The Ugly, The (1966)  3.494949  4.221300  0.726351\n",
    "Kentucky Fried Movie, The (1977)        2.878788  3.555147  0.676359\n",
    "Dumb & Dumber (1994)                    2.697987  3.336595  0.638608\n",
    "Longest Day, The (1962)                 3.411765  4.031447  0.619682\n",
    "Cable Guy, The (1996)                   2.250000  2.863787  0.613787\n",
    "Evil Dead II (Dead By Dawn) (1987)      3.297297  3.909283  0.611985\n",
    "Hidden, The (1987)                      3.137931  3.745098  0.607167\n",
    "Rocky III (1982)                        2.361702  2.943503  0.581801\n",
    "Caddyshack (1980)                       3.396135  3.969737  0.573602\n",
    "For a Few Dollars More (1965)           3.409091  3.953795  0.544704"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d928606",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果只是想要找出分歧最大的电影（不考虑性别因素），则可以计算得分数据的方差或标准差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of rating grouped by title\n",
    "In [91]: rating_std_by_title = data.groupby('title')['rating'].std()\n",
    "\n",
    "# Filter down to active_titles\n",
    "In [92]: rating_std_by_title = rating_std_by_title.loc[active_titles]\n",
    "\n",
    "# Order Series by value in descending order\n",
    "In [93]: rating_std_by_title.sort_values(ascending=False)[:10]\n",
    "Out[93]: \n",
    "title\n",
    "Dumb & Dumber (1994)                     1.321333\n",
    "Blair Witch Project, The (1999)          1.316368\n",
    "Natural Born Killers (1994)              1.307198\n",
    "Tank Girl (1995)                         1.277695\n",
    "Rocky Horror Picture Show, The (1975)    1.260177\n",
    "Eyes Wide Shut (1999)                    1.259624\n",
    "Evita (1996)                             1.253631\n",
    "Billy Madison (1995)                     1.249970\n",
    "Fear and Loathing in Las Vegas (1998)    1.246408\n",
    "Bicentennial Man (1999)                  1.245533\n",
    "Name: rating, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a34b7e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "可能你已经注意到了，电影分类是以竖线（|）分隔的字符串形式给出的。如果想对电影分类进行分析的话，就需要先将其转换成更有用的形式才行。\n",
    "\n",
    "# 14.3 1880-2010年间全美婴儿姓名\n",
    "\n",
    "美国社会保障总署（SSA）提供了一份从1880年到现在的婴儿名字频率数据。Hadley Wickham（许多流行R包的作者）经常用这份数据来演示R的数据处理功能。\n",
    "\n",
    "我们要做一些数据规整才能加载这个数据集，这么做就会产生一个如下的DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [4]: names.head(10)\n",
    "Out[4]:\n",
    "        name sex  births  year\n",
    "0       Mary   F    7065  1880\n",
    "1       Anna   F    2604  1880\n",
    "2       Emma   F    2003  1880\n",
    "3  Elizabeth   F    1939  1880\n",
    "4     Minnie   F    1746  1880\n",
    "5   Margaret   F    1578  1880\n",
    "6        Ida   F    1472  1880\n",
    "7      Alice   F    1414  1880\n",
    "8     Bertha   F    1320  1880\n",
    "9      Sarah   F    1288  1880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d69993",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "你可以用这个数据集做很多事，例如：\n",
    "\n",
    "- 计算指定名字（可以是你自己的，也可以是别人的）的年度比例。\n",
    "- 计算某个名字的相对排名。\n",
    "- 计算各年度最流行的名字，以及增长或减少最快的名字。\n",
    "- 分析名字趋势：元音、辅音、长度、总体多样性、拼写变化、首尾字母等。\n",
    "- 分析外源性趋势：圣经中的名字、名人、人口结构变化等。\n",
    "\n",
    "利用前面介绍过的那些工具，这些分析工作都能很轻松地完成，我会讲解其中的一些。\n",
    "\n",
    "到编写本书时为止，美国社会保障总署将该数据库按年度制成了多个数据文件，其中给出了每个性别/名字组合的出生总数。这些文件的原始档案可以在这里获取：[http://www.ssa.gov/oact/babynames/limits.html](http://www.ssa.gov/oact/babynames/limits.html)。\n",
    "\n",
    "如果你在阅读本书的时候这个页面已经不见了，也可以用搜索引擎找找。\n",
    "\n",
    "下载\"National data\"文件names.zip，解压后的目录中含有一组文件（如yob1880.txt）。我用UNIX的head命令查看了其中一个文件的前10行（在Windows上，你可以用more命令，或直接在文本编辑器中打开）：\n",
    "```\n",
    "In [94]: !head -n 10 datasets/babynames/yob1880.txt\n",
    "Mary,F,7065\n",
    "Anna,F,2604\n",
    "Emma,F,2003\n",
    "Elizabeth,F,1939\n",
    "Minnie,F,1746\n",
    "Margaret,F,1578\n",
    "Ida,F,1472\n",
    "Alice,F,1414\n",
    "Bertha,F,1320\n",
    "Sarah,F,1288\n",
    "```\n",
    "\n",
    "由于这是一个非常标准的以逗号隔开的格式，所以可以用pandas.read_csv将其加载到DataFrame中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [95]: import pandas as pd\n",
    "\n",
    "In [96]: names1880 =\n",
    "pd.read_csv('datasets/babynames/yob1880.txt',\n",
    "   ....:                         names=['name', 'sex', 'births'])\n",
    "\n",
    "In [97]: names1880\n",
    "Out[97]: \n",
    "           name sex  births\n",
    "0          Mary   F    7065\n",
    "1          Anna   F    2604\n",
    "2          Emma   F    2003\n",
    "3     Elizabeth   F    1939\n",
    "4        Minnie   F    1746\n",
    "...         ...  ..     ...\n",
    "1995     Woodie   M       5\n",
    "1996     Worthy   M       5\n",
    "1997     Wright   M       5\n",
    "1998       York   M       5\n",
    "1999  Zachariah   M       5\n",
    "[2000 rows x 3 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fea03",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这些文件中仅含有当年出现超过5次的名字。为了简单起见，我们可以用births列的sex分组小计表示该年度的births总计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b04e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [98]: names1880.groupby('sex').births.sum()\n",
    "Out[98]: \n",
    "sex\n",
    "F     90993\n",
    "M    110493\n",
    "Name: births, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b1205",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "由于该数据集按年度被分隔成了多个文件，所以第一件事情就是要将所有数据都组装到一个DataFrame里面，并加上一个year字段。使用pandas.concat即可达到这个目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e70924",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1880, 2011)\n",
    "\n",
    "pieces = []\n",
    "columns = ['name', 'sex', 'births']\n",
    "\n",
    "for year in years:\n",
    "    path = 'datasets/babynames/yob%d.txt' % year\n",
    "    frame = pd.read_csv(path, names=columns)\n",
    "\n",
    "    frame['year'] = year\n",
    "    pieces.append(frame)\n",
    "\n",
    "# Concatenate everything into a single DataFrame\n",
    "names = pd.concat(pieces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e0066",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这里需要注意几件事情。第一，concat默认是按行将多个DataFrame组合到一起的；第二，必须指定ignore_index=True，因为我们不希望保留read_csv所返回的原始行号。现在我们得到了一个非常大的DataFrame，它含有全部的名字数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461239b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [100]: names\n",
    "Out[100]: \n",
    "              name sex  births  year\n",
    "0             Mary   F    7065  1880\n",
    "1             Anna   F    2604  1880\n",
    "2             Emma   F    2003  1880\n",
    "3        Elizabeth   F    1939  1880\n",
    "4           Minnie   F    1746  1880\n",
    "...            ...  ..     ...   ...\n",
    "1690779    Zymaire   M       5  2010\n",
    "1690780     Zyonne   M       5  2010\n",
    "1690781  Zyquarius   M       5  2010\n",
    "1690782      Zyran   M       5  2010\n",
    "1690783      Zzyzx   M       5  2010\n",
    "[1690784 rows x 4 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18fe64",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "有了这些数据之后，我们就可以利用groupby或pivot_table在year和sex级别上对其进行聚合了，如图14-4所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a22b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [101]: total_births = names.pivot_table('births', index='year',\n",
    "   .....:                                  columns='sex', aggfunc=sum)\n",
    "\n",
    "In [102]: total_births.tail()\n",
    "Out[102]: \n",
    "sex         F        M\n",
    "year                  \n",
    "2006  1896468  2050234\n",
    "2007  1916888  2069242\n",
    "2008  1883645  2032310\n",
    "2009  1827643  1973359\n",
    "2010  1759010  1898382\n",
    "\n",
    "In [103]: total_births.plot(title='Total births by sex and year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9cc6d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-4 按性别和年度统计的总出生数](http://upload-images.jianshu.io/upload_images/7178691-7643b150d88aae11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "下面我们来插入一个prop列，用于存放指定名字的婴儿数相对于总出生数的比例。prop值为0.02表示每100名婴儿中有2名取了当前这个名字。因此，我们先按year和sex分组，然后再将新列加到各个分组上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prop(group):\n",
    "    group['prop'] = group.births / group.births.sum()\n",
    "    return group\n",
    "names = names.groupby(['year', 'sex']).apply(add_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a12833",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，完整的数据集就有了下面这些列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af683abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [105]: names\n",
    "Out[105]: \n",
    "              name sex  births  year      prop\n",
    "0             Mary   F    7065  1880  0.077643\n",
    "1             Anna   F    2604  1880  0.028618\n",
    "2             Emma   F    2003  1880  0.022013\n",
    "3        Elizabeth   F    1939  1880  0.021309\n",
    "4           Minnie   F    1746  1880  0.019188\n",
    "...            ...  ..     ...   ...       ...\n",
    "1690779    Zymaire   M       5  2010  0.000003\n",
    "1690780     Zyonne   M       5  2010  0.000003\n",
    "1690781  Zyquarius   M       5  2010  0.000003\n",
    "1690782      Zyran   M       5  2010  0.000003\n",
    "1690783      Zzyzx   M       5  2010  0.000003\n",
    "[1690784 rows x 5 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534267c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "在执行这样的分组处理时，一般都应该做一些有效性检查，比如验证所有分组的prop的总和是否为1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53581630",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [106]: names.groupby(['year', 'sex']).prop.sum()\n",
    "Out[106]: \n",
    "year  sex\n",
    "1880  F      1.0\n",
    "      M      1.0\n",
    "1881  F      1.0\n",
    "      M      1.0\n",
    "1882  F      1.0\n",
    "            ... \n",
    "2008  M      1.0\n",
    "2009  F      1.0\n",
    "      M      1.0\n",
    "2010  F      1.0\n",
    "      M      1.0\n",
    "Name: prop, Length: 262, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6039e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "工作完成。为了便于实现更进一步的分析，我需要取出该数据的一个子集：每对sex/year组合的前1000个名字。这又是一个分组操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top1000(group):\n",
    "    return group.sort_values(by='births', ascending=False)[:1000]\n",
    "grouped = names.groupby(['year', 'sex'])\n",
    "top1000 = grouped.apply(get_top1000)\n",
    "# Drop the group index, not needed\n",
    "top1000.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092774cb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果你喜欢DIY的话，也可以这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = []\n",
    "for year, group in names.groupby(['year', 'sex']):\n",
    "    pieces.append(group.sort_values(by='births', ascending=False)[:1000])\n",
    "top1000 = pd.concat(pieces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300f42a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在的结果数据集就小多了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c34aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [108]: top1000\n",
    "Out[108]: \n",
    "             name sex  births  year      prop\n",
    "0            Mary   F    7065  1880  0.077643\n",
    "1            Anna   F    2604  1880  0.028618\n",
    "2            Emma   F    2003  1880  0.022013\n",
    "3       Elizabeth   F    1939  1880  0.021309\n",
    "4          Minnie   F    1746  1880  0.019188\n",
    "...           ...  ..     ...   ...       ...\n",
    "261872     Camilo   M     194  2010  0.000102\n",
    "261873     Destin   M     194  2010  0.000102\n",
    "261874     Jaquan   M     194  2010  0.000102\n",
    "261875     Jaydan   M     194  2010  0.000102\n",
    "261876     Maxton   M     193  2010  0.000102\n",
    "[261877 rows x 5 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55781ce9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "接下来的数据分析工作就针对这个top1000数据集了。\n",
    "\n",
    "## 分析命名趋势\n",
    "\n",
    "有了完整的数据集和刚才生成的top1000数据集，我们就可以开始分析各种命名趋势了。首先将前1000个名字分为男女两个部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c618517",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [109]: boys = top1000[top1000.sex == 'M']\n",
    "\n",
    "In [110]: girls = top1000[top1000.sex == 'F']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84823a73",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这是两个简单的时间序列，只需稍作整理即可绘制出相应的图表（比如每年叫做John和Mary的婴儿数）。我们先生成一张按year和name统计的总出生数透视表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [111]: total_births = top1000.pivot_table('births', index='year',\n",
    "   .....:                                    columns='name',\n",
    "   .....:                                    aggfunc=sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c1e67",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，我们用DataFrame的plot方法绘制几个名字的曲线图（见图14-5）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [112]: total_births.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 131 entries, 1880 to 2010\n",
    "Columns: 6868 entries, Aaden to Zuri\n",
    "dtypes: float64(6868)\n",
    "memory usage: 6.9 MB\n",
    "\n",
    "In [113]: subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]\n",
    "\n",
    "In [114]: subset.plot(subplots=True, figsize=(12, 10), grid=False,\n",
    "   .....:             title=\"Number of births per year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f77540",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-5 几个男孩和女孩名字随时间变化的使用数量](http://upload-images.jianshu.io/upload_images/7178691-33f0f97656367a53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "从图中可以看出，这几个名字在美国人民的心目中已经风光不再了。但事实并非如此简单，我们在下一节中就能知道是怎么一回事了。\n",
    "\n",
    "## 评估命名多样性的增长\n",
    "\n",
    "一种解释是父母愿意给小孩起常见的名字越来越少。这个假设可以从数据中得到验证。一个办法是计算最流行的1000个名字所占的比例，我按year和sex进行聚合并绘图（见图14-6）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03465fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [116]: table = top1000.pivot_table('prop', index='year',\n",
    "   .....:                             columns='sex', aggfunc=sum)\n",
    "\n",
    "In [117]: table.plot(title='Sum of table1000.prop by year and sex',\n",
    "   .....:            yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f380e0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-6 分性别统计的前1000个名字在总出生人数中的比例](http://upload-images.jianshu.io/upload_images/7178691-63e1ddc326a033b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "从图中可以看出，名字的多样性确实出现了增长（前1000项的比例降低）。另一个办法是计算占总出生人数前50%的不同名字的数量，这个数字不太好计算。我们只考虑2010年男孩的名字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a325a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [118]: df = boys[boys.year == 2010]\n",
    "\n",
    "In [119]: df\n",
    "Out[119]: \n",
    "           name sex  births  year      prop\n",
    "260877    Jacob   M   21875  2010  0.011523\n",
    "260878    Ethan   M   17866  2010  0.009411\n",
    "260879  Michael   M   17133  2010  0.009025\n",
    "260880   Jayden   M   17030  2010  0.008971\n",
    "260881  William   M   16870  2010  0.008887\n",
    "...         ...  ..     ...   ...       ...\n",
    "261872   Camilo   M     194  2010  0.000102\n",
    "261873   Destin   M     194  2010  0.000102\n",
    "261874   Jaquan   M     194  2010  0.000102\n",
    "261875   Jaydan   M     194  2010  0.000102\n",
    "261876   Maxton   M     193  2010  0.000102\n",
    "[1000 rows x 5 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e462ea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "在对prop降序排列之后，我们想知道前面多少个名字的人数加起来才够50%。虽然编写一个for循环确实也能达到目的，但NumPy有一种更聪明的矢量方式。先计算prop的累计和cumsum，然后再通过searchsorted方法找出0.5应该被插入在哪个位置才能保证不破坏顺序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b964c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [120]: prop_cumsum = df.sort_values(by='prop', ascending=False).prop.cumsum()\n",
    "\n",
    "In [121]: prop_cumsum[:10]\n",
    "Out[121]: \n",
    "260877    0.011523\n",
    "260878    0.020934\n",
    "260879    0.029959\n",
    "260880    0.038930\n",
    "260881    0.047817\n",
    "260882    0.056579\n",
    "260883    0.065155\n",
    "260884    0.073414\n",
    "260885    0.081528\n",
    "260886    0.089621\n",
    "Name: prop, dtype: float64\n",
    "\n",
    "In [122]: prop_cumsum.values.searchsorted(0.5)\n",
    "Out[122]: 116"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed076a2c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "由于数组索引是从0开始的，因此我们要给这个结果加1，即最终结果为117。拿1900年的数据来做个比较，这个数字要小得多："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21245d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [123]: df = boys[boys.year == 1900]\n",
    "\n",
    "In [124]: in1900 = df.sort_values(by='prop', ascending=False).prop.cumsum()\n",
    "\n",
    "In [125]: in1900.values.searchsorted(0.5) + 1\n",
    "Out[125]: 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe438214",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在就可以对所有year/sex组合执行这个计算了。按这两个字段进行groupby处理，然后用一个函数计算各分组的这个值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile_count(group, q=0.5):\n",
    "    group = group.sort_values(by='prop', ascending=False)\n",
    "    return group.prop.cumsum().values.searchsorted(q) + 1\n",
    "\n",
    "diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)\n",
    "diversity = diversity.unstack('sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391bee7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，diversity这个DataFrame拥有两个时间序列（每个性别各一个，按年度索引）。通过IPython，你可以查看其内容，还可以像之前那样绘制图表（如图14-7所示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceebcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [128]: diversity.head()\n",
    "Out[128]: \n",
    "sex    F   M\n",
    "year        \n",
    "1880  38  14\n",
    "1881  38  14\n",
    "1882  38  15\n",
    "1883  39  15\n",
    "1884  39  16\n",
    "\n",
    "In [129]: diversity.plot(title=\"Number of popular names in top 50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773cdb1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-7 按年度统计的密度表](http://upload-images.jianshu.io/upload_images/7178691-574b53a383cad681.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "从图中可以看出，女孩名字的多样性总是比男孩的高，而且还在变得越来越高。读者们可以自己分析一下具体是什么在驱动这个多样性（比如拼写形式的变化）。\n",
    "\n",
    "##  “最后一个字母”的变革\n",
    "\n",
    "2007年，一名婴儿姓名研究人员Laura Wattenberg在她自己的网站上指出（http://www.babynamewizard.com）：近百年来，男孩名字在最后一个字母上的分布发生了显著的变化。为了了解具体的情况，我首先将全部出生数据在年度、性别以及末字母上进行了聚合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract last letter from name column\n",
    "get_last_letter = lambda x: x[-1]\n",
    "last_letters = names.name.map(get_last_letter)\n",
    "last_letters.name = 'last_letter'\n",
    "\n",
    "table = names.pivot_table('births', index=last_letters,\n",
    "                          columns=['sex', 'year'], aggfunc=sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5743995",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后，我选出具有一定代表性的三年，并输出前面几行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [131]: subtable = table.reindex(columns=[1910, 1960, 2010], level='year')\n",
    "\n",
    "In [132]: subtable.head()\n",
    "Out[132]: \n",
    "sex                 F                            M                    \n",
    "year             1910      1960      2010     1910      1960      2010\n",
    "last_letter                                                           \n",
    "a            108376.0  691247.0  670605.0    977.0    5204.0   28438.0\n",
    "b                 NaN     694.0     450.0    411.0    3912.0   38859.0\n",
    "c                 5.0      49.0     946.0    482.0   15476.0   23125.0\n",
    "d              6750.0    3729.0    2607.0  22111.0  262112.0   44398.0\n",
    "e            133569.0  435013.0  313833.0  28655.0  178823.0  129012.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd2a34",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "接下来我们需要按总出生数对该表进行规范化处理，以便计算出各性别各末字母占总出生人数的比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [133]: subtable.sum()\n",
    "Out[133]: \n",
    "sex  year\n",
    "F    1910     396416.0\n",
    "     1960    2022062.0\n",
    "     2010    1759010.0\n",
    "M    1910     194198.0\n",
    "     1960    2132588.0\n",
    "2010    1898382.0\n",
    "dtype: float64\n",
    "\n",
    "In [134]: letter_prop = subtable / subtable.sum()\n",
    "\n",
    "In [135]: letter_prop\n",
    "Out[135]: \n",
    "sex                 F                             M                    \n",
    "year             1910      1960      2010      1910      1960      2010\n",
    "last_letter                                                            \n",
    "a            0.273390  0.341853  0.381240  0.005031  0.002440  0.014980\n",
    "b                 NaN  0.000343  0.000256  0.002116  0.001834  0.020470\n",
    "c            0.000013  0.000024  0.000538  0.002482  0.007257  0.012181\n",
    "d            0.017028  0.001844  0.001482  0.113858  0.122908  0.023387\n",
    "e            0.336941  0.215133  0.178415  0.147556  0.083853  0.067959\n",
    "...               ...       ...       ...       ...       ...       ...\n",
    "v                 NaN  0.000060  0.000117  0.000113\n",
    "0.000037  0.001434\n",
    "w            0.000020  0.000031  0.001182  0.006329  0.007711  0.016148\n",
    "x            0.000015  0.000037  0.000727  0.003965  0.001851  0.008614\n",
    "y            0.110972  0.152569  0.116828  0.077349  0.160987  0.058168\n",
    "z            0.002439  0.000659  0.000704  0.000170  0.000184  0.001831\n",
    "[26 rows x 6 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dea432",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "有了这个字母比例数据之后，就可以生成一张各年度各性别的条形图了，如图14-8所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')\n",
    "letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female',\n",
    "                      legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83241bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-8 男孩女孩名字中各个末字母的比例](http://upload-images.jianshu.io/upload_images/7178691-67686f38e66ef5f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "可以看出，从20世纪60年代开始，以字母\"n\"结尾的男孩名字出现了显著的增长。回到之前创建的那个完整表，按年度和性别对其进行规范化处理，并在男孩名字中选取几个字母，最后进行转置以便将各个列做成一个时间序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24443f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [138]: letter_prop = table / table.sum()\n",
    "\n",
    "In [139]: dny_ts = letter_prop.loc[['d', 'n', 'y'], 'M'].T\n",
    "\n",
    "In [140]: dny_ts.head()\n",
    "Out[140]: \n",
    "last_letter         d         n         y\n",
    "year                                     \n",
    "1880         0.083055  0.153213  0.075760\n",
    "1881         0.083247  0.153214  0.077451\n",
    "1882         0.085340  0.149560  0.077537\n",
    "1883         0.084066  0.151646  0.079144\n",
    "1884         0.086120  0.149915  0.080405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a3abb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "有了这个时间序列的DataFrame之后，就可以通过其plot方法绘制出一张趋势图了（如图14-9所示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [143]: dny_ts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2850e1",
   "metadata": {},
   "source": [
    "![图14-9 各年出生的男孩中名字以d/n/y结尾的人数比例](http://upload-images.jianshu.io/upload_images/7178691-51c431b2490424c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## 变成女孩名字的男孩名字（以及相反的情况）\n",
    "\n",
    "另一个有趣的趋势是，早年流行于男孩的名字近年来“变性了”，例如Lesley或Leslie。回到top1000数据集，找出其中以\"lesl\"开头的一组名字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [144]: all_names = pd.Series(top1000.name.unique())\n",
    "\n",
    "In [145]: lesley_like = all_names[all_names.str.lower().str.contains('lesl')]\n",
    "\n",
    "In [146]: lesley_like\n",
    "Out[146]: \n",
    "632     Leslie\n",
    "2294    Lesley\n",
    "4262    Leslee\n",
    "4728     Lesli\n",
    "6103     Lesly\n",
    "dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265f297",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后利用这个结果过滤其他的名字，并按名字分组计算出生数以查看相对频率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [147]: filtered = top1000[top1000.name.isin(lesley_like)]\n",
    "\n",
    "In [148]: filtered.groupby('name').births.sum()\n",
    "Out[148]: \n",
    "name\n",
    "Leslee      1082\n",
    "Lesley     35022\n",
    "Lesli        929\n",
    "Leslie    370429\n",
    "Lesly      10067\n",
    "Name: births, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ec8b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "接下来，我们按性别和年度进行聚合，并按年度进行规范化处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [149]: table = filtered.pivot_table('births', index='year',\n",
    "   .....:                              columns='sex', aggfunc='sum')\n",
    "\n",
    "In [150]: table = table.div(table.sum(1), axis=0)\n",
    "\n",
    "In [151]: table.tail()\n",
    "Out[151]: \n",
    "sex     F   M\n",
    "year         \n",
    "2006  1.0 NaN\n",
    "2007  1.0 NaN\n",
    "2008  1.0 NaN\n",
    "2009  1.0 NaN\n",
    "2010  1.0 NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0948244",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "最后，就可以轻松绘制一张分性别的年度曲线图了（如图2-10所示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91084bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [153]: table.plot(style={'M': 'k-', 'F': 'k--'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07dc079",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-10 各年度使用“Lesley型”名字的男女比例](http://upload-images.jianshu.io/upload_images/7178691-b99d98f8bb5fc695.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "# 14.4 USDA食品数据库\n",
    "\n",
    "美国农业部（USDA）制作了一份有关食物营养信息的数据库。Ashley Williams制作了该数据的JSON版（http://ashleyw.co.uk/project/food-nutrient-database）。其中的记录如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": 21441,\n",
    "  \"description\": \"KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,\n",
    "Wing, meat and skin with breading\",\n",
    "  \"tags\": [\"KFC\"],\n",
    "  \"manufacturer\": \"Kentucky Fried Chicken\",\n",
    "\"group\": \"Fast Foods\",\n",
    "  \"portions\": [\n",
    "    {\n",
    "      \"amount\": 1,\n",
    "      \"unit\": \"wing, with skin\",\n",
    "      \"grams\": 68.0\n",
    "    },\n",
    "\n",
    "    ...\n",
    "  ],\n",
    "  \"nutrients\": [\n",
    "    {\n",
    "      \"value\": 20.8,\n",
    "      \"units\": \"g\",\n",
    "      \"description\": \"Protein\",\n",
    "      \"group\": \"Composition\"\n",
    "    },\n",
    "\n",
    "    ...\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee09efa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "每种食物都带有若干标识性属性以及两个有关营养成分和分量的列表。这种形式的数据不是很适合分析工作，因此我们需要做一些规整化以使其具有更好用的形式。\n",
    "\n",
    "从上面列举的那个网址下载并解压数据之后，你可以用任何喜欢的JSON库将其加载到Python中。我用的是Python内置的json模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b582d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [154]: import json\n",
    "\n",
    "In [155]: db = json.load(open('datasets/usda_food/database.json'))\n",
    "\n",
    "In [156]: len(db)\n",
    "Out[156]: 6636"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d0480",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "db中的每个条目都是一个含有某种食物全部数据的字典。nutrients字段是一个字典列表，其中的每个字典对应一种营养成分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [157]: db[0].keys()\n",
    "Out[157]: dict_keys(['id', 'description', 'tags', 'manufacturer', 'group', 'porti\n",
    "ons', 'nutrients'])\n",
    "\n",
    "In [158]: db[0]['nutrients'][0]\n",
    "Out[158]: \n",
    "{'description': 'Protein',\n",
    " 'group': 'Composition',\n",
    " 'units': 'g',\n",
    " 'value': 25.18}\n",
    "\n",
    "In [159]: nutrients = pd.DataFrame(db[0]['nutrients'])\n",
    "\n",
    "In [160]: nutrients[:7]\n",
    "Out[160]: \n",
    "                   description        group units    value\n",
    "0                      Protein  Composition     g    25.18\n",
    "1            Total lipid (fat)  Composition     g    29.20\n",
    "2  Carbohydrate, by difference  Composition     g     3.06\n",
    "3                          Ash        Other     g     3.28\n",
    "4                       Energy       Energy  kcal   376.00\n",
    "5                        Water  Composition     g    39.28\n",
    "6                       Energy       Energy    kJ  1573.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dee907",
   "metadata": {},
   "source": [
    "在将字典列表转换为DataFrame时，可以只抽取其中的一部分字段。这里，我们将取出食物的名称、分类、编号以及制造商等信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [161]: info_keys = ['description', 'group', 'id', 'manufacturer']\n",
    "\n",
    "In [162]: info = pd.DataFrame(db, columns=info_keys)\n",
    "\n",
    "In [163]: info[:5]\n",
    "Out[163]: \n",
    "                          description                   group    id  \\\n",
    "0                     Cheese, caraway  Dairy and Egg Products  1008   \n",
    "1                     Cheese, cheddar  Dairy and Egg Products  1009\n",
    "2                        Cheese, edam  Dairy and Egg Products  1018   \n",
    "3                        Cheese, feta  Dairy and Egg Products  1019   \n",
    "4  Cheese, mozzarella, part skim milk  Dairy and Egg Products  1028   \n",
    "  manufacturer  \n",
    "0               \n",
    "1               \n",
    "2               \n",
    "3               \n",
    "4               \n",
    "\n",
    "In [164]: info.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 6636 entries, 0 to 6635\n",
    "Data columns (total 4 columns):\n",
    "description     6636 non-null object\n",
    "group           6636 non-null object\n",
    "id              6636 non-null int64\n",
    "manufacturer    5195 non-null object\n",
    "dtypes: int64(1), object(3)\n",
    "memory usage: 207.5+ KB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b5dad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "通过value_counts，你可以查看食物类别的分布情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf78bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [165]: pd.value_counts(info.group)[:10]\n",
    "Out[165]: \n",
    "Vegetables and Vegetable Products    812\n",
    "Beef Products                        618\n",
    "Baked Products                       496\n",
    "Breakfast Cereals                    403\n",
    "Fast Foods                           365\n",
    "Legumes and Legume Products          365\n",
    "Lamb, Veal, and Game Products        345\n",
    "Sweets                               341\n",
    "Pork Products                        328\n",
    "Fruits and Fruit Juices              328\n",
    "Name: group, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c347b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，为了对全部营养数据做一些分析，最简单的办法是将所有食物的营养成分整合到一个大表中。我们分几个步骤来实现该目的。首先，将各食物的营养成分列表转换为一个DataFrame，并添加一个表示编号的列，然后将该DataFrame添加到一个列表中。最后通过concat将这些东西连接起来就可以了：\n",
    "\n",
    "顺利的话，nutrients的结果是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [167]: nutrients\n",
    "Out[167]: \n",
    "                               description        group units    value     id\n",
    "0                                  Protein  Composition     g   25.180   1008\n",
    "1                        Total lipid (fat)  Composition     g   29.200   1008\n",
    "2              Carbohydrate, by difference  Composition     g    3.060   1008\n",
    "3                                      Ash        Other     g    3.280   1008\n",
    "4                                   Energy       Energy  kcal  376.000   1008\n",
    "...                                    ...          ...\n",
    "...      ...    ...\n",
    "389350                 Vitamin B-12, added     Vitamins   mcg    0.000  43546\n",
    "389351                         Cholesterol        Other    mg    0.000  43546\n",
    "389352        Fatty acids, total saturated        Other     g    0.072  43546\n",
    "389353  Fatty acids, total monounsaturated        Other     g    0.028  43546\n",
    "389354  Fatty acids, total polyunsaturated        Other     g    0.041  43546\n",
    "[389355 rows x 5 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079db816",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "我发现这个DataFrame中无论如何都会有一些重复项，所以直接丢弃就可以了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01075f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [168]: nutrients.duplicated().sum()  # number of duplicates\n",
    "Out[168]: 14179\n",
    "\n",
    "In [169]: nutrients = nutrients.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49770ff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "由于两个DataFrame对象中都有\"group\"和\"description\"，所以为了明确到底谁是谁，我们需要对它们进行重命名："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f11d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [170]: col_mapping = {'description' : 'food',\n",
    "   .....:                'group'       : 'fgroup'}\n",
    "\n",
    "In [171]: info = info.rename(columns=col_mapping, copy=False)\n",
    "\n",
    "In [172]: info.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 6636 entries, 0 to 6635\n",
    "Data columns (total 4 columns):\n",
    "food            6636 non-null object\n",
    "fgroup          6636 non-null object\n",
    "id              6636 non-null int64\n",
    "manufacturer    5195 non-null object\n",
    "dtypes: int64(1), object(3)\n",
    "memory usage: 207.5+ KB\n",
    "\n",
    "In [173]: col_mapping = {'description' : 'nutrient',\n",
    "   .....:                'group' : 'nutgroup'}\n",
    "In [174]: nutrients = nutrients.rename(columns=col_mapping, copy=False)\n",
    "\n",
    "In [175]: nutrients\n",
    "Out[175]: \n",
    "                                  nutrient     nutgroup units    value     id\n",
    "0                                  Protein  Composition     g   25.180   1008\n",
    "1                        Total lipid (fat)  Composition     g   29.200   1008\n",
    "2              Carbohydrate, by difference  Composition     g    3.060   1008\n",
    "3                                      Ash        Other     g    3.280   1008\n",
    "4                                   Energy       Energy  kcal  376.000   1008\n",
    "...                                    ...          ...   ...      ...    ...\n",
    "389350                 Vitamin B-12, added     Vitamins   mcg    0.000  43546\n",
    "389351                         Cholesterol        Other    mg    0.000  43546\n",
    "389352        Fatty acids, total saturated        Other     g    0.072  43546\n",
    "389353  Fatty acids, total monounsaturated        Other     g    0.028  43546\n",
    "389354  Fatty acids, total polyunsaturated        Other     g    0.041  43546\n",
    "[375176 rows x 5 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1b802",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "做完这些，就可以将info跟nutrients合并起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c70bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [176]: ndata = pd.merge(nutrients, info, on='id', how='outer')\n",
    "\n",
    "In [177]: ndata.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 375176 entries, 0 to 375175\n",
    "Data columns (total 8 columns):\n",
    "nutrient        375176 non-null object\n",
    "nutgroup        375176 non-null object\n",
    "units           375176 non-null object\n",
    "value           375176 non-null float64\n",
    "id              375176 non-null int64\n",
    "food            375176 non-null object\n",
    "fgroup          375176 non-null object\n",
    "manufacturer    293054 non-null object\n",
    "dtypes: float64(1), int64(1), object(6)\n",
    "memory usage: 25.8+ MB\n",
    "\n",
    "In [178]: ndata.iloc[30000]\n",
    "Out[178]: \n",
    "nutrient                                       Glycine\n",
    "nutgroup                                   Amino Acids\n",
    "units                                                g\n",
    "value                                             0.04\n",
    "id                                                6158\n",
    "food            Soup, tomato bisque, canned, condensed\n",
    "fgroup                      Soups, Sauces, and Gravies\n",
    "manufacturer                                          \n",
    "Name: 30000, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5add7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "我们现在可以根据食物分类和营养类型画出一张中位值图（如图14-11所示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2552ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [180]: result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)\n",
    "\n",
    "In [181]: result['Zinc, Zn'].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22867121",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图片14-11 根据营养分类得出的锌中位值](http://upload-images.jianshu.io/upload_images/7178691-99b176d022a444c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "只要稍微动一动脑子，就可以发现各营养成分最为丰富的食物是什么了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed26f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])\n",
    "\n",
    "get_maximum = lambda x: x.loc[x.value.idxmax()]\n",
    "get_minimum = lambda x: x.loc[x.value.idxmin()]\n",
    "\n",
    "max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]\n",
    "\n",
    "# make the food a little smaller\n",
    "max_foods.food = max_foods.food.str[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399aff2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "由于得到的DataFrame很大，所以不方便在书里面全部打印出来。这里只给出\"Amino Acids\"营养分组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [183]: max_foods.loc['Amino Acids']['food']\n",
    "Out[183]: \n",
    "nutrient\n",
    "Alanine                          Gelatins, dry powder, unsweetened\n",
    "Arginine                              Seeds, sesame flour, low-fat\n",
    "Aspartic acid                                  Soy protein isolate\n",
    "Cystine               Seeds, cottonseed flour, low fat (glandless)\n",
    "Glutamic acid                                  Soy protein isolate\n",
    "                                       ...                        \n",
    "Serine           Soy protein isolate, PROTEIN TECHNOLOGIES INTE...\n",
    "Threonine        Soy protein isolate, PROTEIN TECHNOLOGIES INTE...\n",
    "Tryptophan        Sea lion, Steller, meat with fat (Alaska Native)\n",
    "Tyrosine         Soy protein isolate, PROTEIN TECHNOLOGIES INTE...\n",
    "Valine           Soy protein isolate, PROTEIN TECHNOLOGIES INTE...\n",
    "Name: food, Length: 19, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4a5b7",
   "metadata": {},
   "source": [
    "# 14.5 2012联邦选举委员会数据库\n",
    "\n",
    "美国联邦选举委员会发布了有关政治竞选赞助方面的数据。其中包括赞助者的姓名、职业、雇主、地址以及出资额等信息。我们对2012年美国总统大选的数据集比较感兴趣（http://www.fec.gov/disclosurep/PDownload.do）。我在2012年6月下载的数据集是一个150MB的CSV文件（P00000001-ALL.csv），我们先用pandas.read_csv将其加载进来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [184]: fec = pd.read_csv('datasets/fec/P00000001-ALL.csv')\n",
    "\n",
    "In [185]: fec.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1001731 entries, 0 to 1001730\n",
    "Data columns (total 16 columns):\n",
    "cmte_id              1001731 non-null object\n",
    "cand_id              1001731 non-null object\n",
    "cand_nm              1001731 non-null object\n",
    "contbr_nm            1001731 non-null object\n",
    "contbr_city          1001712 non-null object\n",
    "contbr_st            1001727 non-null object\n",
    "contbr_zip           1001620 non-null object\n",
    "contbr_employer      988002 non-null object\n",
    "contbr_occupation    993301 non-null object\n",
    "contb_receipt_amt    1001731 non-null float64\n",
    "contb_receipt_dt     1001731 non-null object\n",
    "receipt_desc         14166 non-null object\n",
    "memo_cd              92482 non-null object\n",
    "memo_text            97770 non-null object\n",
    "form_tp              1001731 non-null object\n",
    "file_num             1001731 non-null int64\n",
    "dtypes: float64(1), int64(1), object(14)\n",
    "memory usage: 122.3+ MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb962e0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "该DataFrame中的记录如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8198a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [186]: fec.iloc[123456]\n",
    "Out[186]: \n",
    "cmte_id             C00431445\n",
    "cand_id             P80003338\n",
    "cand_nm         Obama, Barack\n",
    "contbr_nm         ELLMAN, IRA\n",
    "contbr_city             TEMPE\n",
    "                    ...      \n",
    "receipt_desc              NaN\n",
    "memo_cd                   NaN\n",
    "memo_text                 NaN\n",
    "form_tp                 SA17A\n",
    "file_num               772372\n",
    "Name: 123456, Length: 16, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1319863",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "你可能已经想出了许多办法从这些竞选赞助数据中抽取有关赞助人和赞助模式的统计信息。我将在接下来的内容中介绍几种不同的分析工作（运用到目前为止已经学到的方法）。\n",
    "\n",
    "不难看出，该数据中没有党派信息，因此最好把它加进去。通过unique，你可以获取全部的候选人名单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [187]: unique_cands = fec.cand_nm.unique()\n",
    "\n",
    "In [188]: unique_cands\n",
    "Out[188]: \n",
    "array(['Bachmann, Michelle', 'Romney, Mitt', 'Obama, Barack',\n",
    "       \"Roemer, Charles E. 'Buddy' III\", 'Pawlenty, Timothy',\n",
    "       'Johnson, Gary Earl', 'Paul, Ron', 'Santorum, Rick', 'Cain, Herman',\n",
    "       'Gingrich, Newt', 'McCotter, Thaddeus G', 'Huntsman, Jon',\n",
    "       'Perry, Rick'], dtype=object)\n",
    "\n",
    "In [189]: unique_cands[2]\n",
    "Out[189]: 'Obama, Barack'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e0394",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "指明党派信息的方法之一是使用字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18301965",
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = {'Bachmann, Michelle': 'Republican',\n",
    "           'Cain, Herman': 'Republican',\n",
    "           'Gingrich, Newt': 'Republican',\n",
    "           'Huntsman, Jon': 'Republican',\n",
    "           'Johnson, Gary Earl': 'Republican',\n",
    "           'McCotter, Thaddeus G': 'Republican',\n",
    "           'Obama, Barack': 'Democrat',\n",
    "           'Paul, Ron': 'Republican',\n",
    "           'Pawlenty, Timothy': 'Republican',\n",
    "           'Perry, Rick': 'Republican',\n",
    "           \"Roemer, Charles E. 'Buddy' III\": 'Republican',\n",
    "           'Romney, Mitt': 'Republican',\n",
    "           'Santorum, Rick': 'Republican'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c888f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，通过这个映射以及Series对象的map方法，你可以根据候选人姓名得到一组党派信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [191]: fec.cand_nm[123456:123461]\n",
    "Out[191]: \n",
    "123456    Obama, Barack\n",
    "123457    Obama, Barack\n",
    "123458    Obama, Barack\n",
    "123459    Obama, Barack\n",
    "123460    Obama, Barack\n",
    "Name: cand_nm, dtype: object\n",
    "\n",
    "In [192]: fec.cand_nm[123456:123461].map(parties)\n",
    "Out[192]: \n",
    "123456    Democrat\n",
    "123457    Democrat\n",
    "123458    Democrat\n",
    "123459    Democrat\n",
    "123460    Democrat\n",
    "Name: cand_nm, dtype: object\n",
    "\n",
    "# Add it as a column\n",
    "In [193]: fec['party'] = fec.cand_nm.map(parties)\n",
    "\n",
    "In [194]: fec['party'].value_counts()\n",
    "Out[194]: \n",
    "Democrat      593746\n",
    "Republican    407985\n",
    "Name: party, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8cb7b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "这里有两个需要注意的地方。第一，该数据既包括赞助也包括退款（负的出资额）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceeb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [195]: (fec.contb_receipt_amt > 0).value_counts()\n",
    "Out[195]: \n",
    "True     991475\n",
    "False     10256\n",
    "Name: contb_receipt_amt, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4950bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "为了简化分析过程，我限定该数据集只能有正的出资额："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [196]: fec = fec[fec.contb_receipt_amt > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fbd65d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "由于Barack Obama和Mitt Romney是最主要的两名候选人，所以我还专门准备了一个子集，只包含针对他们两人的竞选活动的赞助信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63227046",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [197]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack','Romney, Mitt'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f4f3f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 根据职业和雇主统计赞助信息\n",
    "\n",
    "基于职业的赞助信息统计是另一种经常被研究的统计任务。例如，律师们更倾向于资助民主党，而企业主则更倾向于资助共和党。你可以不相信我，自己看那些数据就知道了。首先，根据职业计算出资总额，这很简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [198]: fec.contbr_occupation.value_counts()[:10]\n",
    "Out[198]: \n",
    "RETIRED                                   233990\n",
    "INFORMATION REQUESTED                      35107\n",
    "ATTORNEY                                   34286\n",
    "HOMEMAKER                                  29931\n",
    "PHYSICIAN                                  23432\n",
    "INFORMATION REQUESTED PER BEST EFFORTS     21138\n",
    "ENGINEER                                   14334\n",
    "TEACHER                                    13990\n",
    "CONSULTANT                                 13273\n",
    "PROFESSOR                                  12555\n",
    "Name: contbr_occupation, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33428c4d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "不难看出，许多职业都涉及相同的基本工作类型，或者同一样东西有多种变体。下面的代码片段可以清理一些这样的数据（将一个职业信息映射到另一个）。注意，这里巧妙地利用了dict.get，它允许没有映射关系的职业也能“通过”："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_mapping = {\n",
    "   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\n",
    "   'INFORMATION REQUESTED' : 'NOT PROVIDED',\n",
    "   'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED',\n",
    "   'C.E.O.': 'CEO'\n",
    "}\n",
    "\n",
    "# If no mapping provided, return x\n",
    "f = lambda x: occ_mapping.get(x, x)\n",
    "fec.contbr_occupation = fec.contbr_occupation.map(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0576c96",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "我对雇主信息也进行了同样的处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f071894",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mapping = {\n",
    "   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\n",
    "   'INFORMATION REQUESTED' : 'NOT PROVIDED',\n",
    "   'SELF' : 'SELF-EMPLOYED',\n",
    "   'SELF EMPLOYED' : 'SELF-EMPLOYED',\n",
    "}\n",
    "\n",
    "# If no mapping provided, return x\n",
    "f = lambda x: emp_mapping.get(x, x)\n",
    "fec.contbr_employer = fec.contbr_employer.map(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3f82a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在，你可以通过pivot_table根据党派和职业对数据进行聚合，然后过滤掉总出资额不足200万美元的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [201]: by_occupation = fec.pivot_table('contb_receipt_amt',\n",
    "   .....:                                 index='contbr_occupation',\n",
    "   .....:                                 columns='party', aggfunc='sum')\n",
    "\n",
    "In [202]: over_2mm = by_occupation[by_occupation.sum(1) > 2000000]\n",
    "\n",
    "In [203]: over_2mm\n",
    "Out[203]: \n",
    "party                 Democrat    Republican\n",
    "contbr_occupation                           \n",
    "ATTORNEY           11141982.97  7.477194e+06\n",
    "CEO                 2074974.79  4.211041e+06\n",
    "CONSULTANT          2459912.71  2.544725e+06\n",
    "ENGINEER             951525.55  1.818374e+06\n",
    "EXECUTIVE           1355161.05  4.138850e+06\n",
    "...                        ...           ...\n",
    "PRESIDENT           1878509.95  4.720924e+06\n",
    "PROFESSOR           2165071.08  2.967027e+05\n",
    "REAL ESTATE          528902.09  1.625902e+06\n",
    "RETIRED            25305116.38  2.356124e+07\n",
    "SELF-EMPLOYED        672393.40  1.640253e+06\n",
    "[17 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b150dc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "把这些数据做成柱状图看起来会更加清楚（'barh'表示水平柱状图，如图14-12所示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [205]: over_2mm.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bfa17",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-12 对各党派总出资额最高的职业](http://upload-images.jianshu.io/upload_images/7178691-d2254e547c6ce537.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "你可能还想了解一下对Obama和Romney总出资额最高的职业和企业。为此，我们先对候选人进行分组，然后使用本章前面介绍的类似top的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_amounts(group, key, n=5):\n",
    "    totals = group.groupby(key)['contb_receipt_amt'].sum()\n",
    "    return totals.nlargest(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e013ac6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "然后根据职业和雇主进行聚合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [207]: grouped = fec_mrbo.groupby('cand_nm')\n",
    "\n",
    "In [208]: grouped.apply(get_top_amounts, 'contbr_occupation', n=7)\n",
    "Out[208]: \n",
    "cand_nm        contbr_occupation    \n",
    "Obama, Barack  RETIRED                  25305116.38\n",
    "               ATTORNEY                 11141982.97\n",
    "               INFORMATION REQUESTED     4866973.96\n",
    "               HOMEMAKER                 4248875.80\n",
    "               PHYSICIAN                 3735124.94\n",
    "                                           ...     \n",
    "Romney, Mitt   HOMEMAKER                 8147446.22\n",
    "               ATTORNEY                  5364718.82\n",
    "               PRESIDENT                 2491244.89\n",
    "               EXECUTIVE                 2300947.03\n",
    "               C.E.O.                    1968386.11\n",
    "Name: contb_receipt_amt, Length: 14, dtype: float64\n",
    "\n",
    "In [209]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)\n",
    "Out[209]: \n",
    "cand_nm        contbr_employer      \n",
    "Obama, Barack  RETIRED                  22694358.85\n",
    "               SELF-EMPLOYED            17080985.96\n",
    "               NOT EMPLOYED              8586308.70\n",
    "               INFORMATION REQUESTED     5053480.37\n",
    "               HOMEMAKER                 2605408.54\n",
    "                                           ...     \n",
    "Romney, Mitt   CREDIT SUISSE              281150.00\n",
    "               MORGAN STANLEY             267266.00\n",
    "               GOLDMAN SACH & CO.         238250.00\n",
    "               BARCLAYS CAPITAL           162750.00\n",
    "               H.I.G. CAPITAL             139500.00\n",
    "Name: contb_receipt_amt, Length: 20, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e9ce6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 对出资额分组\n",
    "\n",
    "还可以对该数据做另一种非常实用的分析：利用cut函数根据出资额的大小将数据离散化到多个面元中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ebca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [210]: bins = np.array([0, 1, 10, 100, 1000, 10000,\n",
    "   .....:                  100000, 1000000, 10000000])\n",
    "\n",
    "In [211]: labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)\n",
    "\n",
    "In [212]: labels\n",
    "Out[212]: \n",
    "411         (10, 100]\n",
    "412       (100, 1000]\n",
    "413       (100, 1000]\n",
    "414         (10, 100]\n",
    "415         (10, 100]\n",
    "             ...     \n",
    "701381      (10, 100]\n",
    "701382    (100, 1000]\n",
    "701383        (1, 10]\n",
    "701384      (10, 100]\n",
    "701385    (100, 1000]\n",
    "Name: contb_receipt_amt, Length: 694282, dtype: category\n",
    "Categories (8, interval[int64]): [(0, 1] < (1, 10] < (10, 100] < (100, 1000] < (1\n",
    "000, 10000] <\n",
    "                                  (10000, 100000] < (100000, 1000000] < (1000000,\n",
    " 10000000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb6602",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "现在可以根据候选人姓名以及面元标签对奥巴马和罗姆尼数据进行分组，以得到一个柱状图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [213]: grouped = fec_mrbo.groupby(['cand_nm', labels])\n",
    "\n",
    "In [214]: grouped.size().unstack(0)\n",
    "Out[214]: \n",
    "cand_nm              Obama, Barack  Romney, Mitt\n",
    "contb_receipt_amt                               \n",
    "(0, 1]                       493.0          77.0\n",
    "(1, 10]                    40070.0        3681.0\n",
    "(10, 100]                 372280.0       31853.0\n",
    "(100, 1000]               153991.0       43357.0\n",
    "(1000, 10000]              22284.0       26186.0\n",
    "(10000, 100000]                2.0           1.0\n",
    "(100000, 1000000]              3.0           NaN\n",
    "(1000000, 10000000]            4.0           NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6c78b",
   "metadata": {},
   "source": [
    "从这个数据中可以看出，在小额赞助方面，Obama获得的数量比Romney多得多。你还可以对出资额求和并在面元内规格化，以便图形化显示两位候选人各种赞助额度的比例（见图14-13）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2de5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [216]: bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)\n",
    "\n",
    "In [217]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)\n",
    "\n",
    "In [218]: normed_sums\n",
    "Out[218]: \n",
    "cand_nm              Obama, Barack  Romney, Mitt\n",
    "contb_receipt_amt                               \n",
    "(0, 1]                    0.805182      0.194818\n",
    "(1, 10]                   0.918767      0.081233\n",
    "(10, 100]                 0.910769      0.089231\n",
    "(100, 1000]               0.710176      0.289824\n",
    "(1000, 10000]             0.447326      0.552674\n",
    "(10000, 100000]           0.823120      0.176880\n",
    "(100000, 1000000]         1.000000           NaN\n",
    "(1000000, 10000000]       1.000000           NaN\n",
    "\n",
    "In [219]: normed_sums[:-2].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e45c18",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![图14-13 两位候选人收到的各种捐赠额度的总额比例](http://upload-images.jianshu.io/upload_images/7178691-77e8c8d3c784692b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "我排除了两个最大的面元，因为这些不是由个人捐赠的。\n",
    "\n",
    "还可以对该分析过程做许多的提炼和改进。比如说，可以根据赞助人的姓名和邮编对数据进行聚合，以便找出哪些人进行了多次小额捐款，哪些人又进行了一次或多次大额捐款。我强烈建议你下载这些数据并自己摸索一下。\n",
    "\n",
    "## 根据州统计赞助信息\n",
    "\n",
    "根据候选人和州对数据进行聚合是常规操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2add513",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [220]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])\n",
    "\n",
    "In [221]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)\n",
    "\n",
    "In [222]: totals = totals[totals.sum(1) > 100000]\n",
    "\n",
    "In [223]: totals[:10]\n",
    "Out[223]: \n",
    "cand_nm    Obama, Barack  Romney, Mitt\n",
    "contbr_st                             \n",
    "AK             281840.15      86204.24\n",
    "AL             543123.48     527303.51\n",
    "AR             359247.28     105556.00\n",
    "AZ            1506476.98    1888436.23\n",
    "CA           23824984.24   11237636.60\n",
    "CO            2132429.49    1506714.12\n",
    "CT            2068291.26    3499475.45\n",
    "DC            4373538.80    1025137.50\n",
    "DE             336669.14      82712.00\n",
    "FL            7318178.58    8338458.81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718026fe",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "如果对各行除以总赞助额，就会得到各候选人在各州的总赞助额比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [224]: percent = totals.div(totals.sum(1), axis=0)\n",
    "\n",
    "In [225]: percent[:10]\n",
    "Out[225]: \n",
    "cand_nm    Obama, Barack  Romney, Mitt\n",
    "contbr_st                             \n",
    "AK              0.765778      0.234222\n",
    "AL              0.507390      0.492610\n",
    "AR              0.772902      0.227098\n",
    "AZ              0.443745      0.556255\n",
    "CA              0.679498      0.320502\n",
    "CO              0.585970      0.414030\n",
    "CT              0.371476      0.628524\n",
    "DC              0.810113      0.189887\n",
    "DE              0.802776      0.197224\n",
    "FL              0.467417      0.532583"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cbc4d7",
   "metadata": {},
   "source": [
    "#14.6 总结\n",
    "\n",
    "我们已经完成了正文的最后一章。附录中有一些额外的内容，可能对你有用。\n",
    "\n",
    "本书第一版出版已经有5年了，Python已经成为了一个流行的、广泛使用的数据分析语言。你从本书中学到的方法，在相当长的一段时间都是可用的。我希望本书介绍的工具和库对你的工作有用。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
